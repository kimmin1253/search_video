{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11037554,"sourceType":"datasetVersion","datasetId":6874872},{"sourceId":11037559,"sourceType":"datasetVersion","datasetId":6874876},{"sourceId":11039466,"sourceType":"datasetVersion","datasetId":6876224},{"sourceId":11067295,"sourceType":"datasetVersion","datasetId":6896475}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install decord -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2ForConditionalGeneration, AutoProcessor\nimport torch\nfrom PIL import Image\nimport os\nimport sys\nimport decord\nfrom decord import VideoReader, cpu\nfrom transformers import BitsAndBytesConfig\nimport numpy as np\n# sys.path.append(\"/kaggle/input/eilev-git/EILEV-main/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kimmin1253/testvideo-ive\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /kaggle/input/eilev-git/EILEV-main/eilev/\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from accelerate import infer_auto_device_map\n\n# ✅ 모델 로드 시 `device_map` 명시적으로 지정\ndevice_map = infer_auto_device_map(model, max_memory={0: \"8GB\", 1: \"8GB\"}, no_split_module_classes=[\"language_model\"])\n\n# ✅ EILeV 모델 경로\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ✅ 모델 로드 (device_map 수정)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map= device_map, #{\"\": 0},  # 🔥 명확하게 GPU 0번에 로드\n    quantization_config = BitsAndBytesConfig(load_in_8bit=True),\n    torch_dtype=torch.float16,  # 메모리 최적화\n)\n# ✅ `language_model`이 device_map에 포함되지 않으면 수동으로 분배\n# model = dispatch_model(model, device_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"✅ Loaded Model Class: {type(model)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ BLIP-2의 입력 처리 방식 적용\n# ✅ 비디오 처리 및 입력 변환\n\nfrom eilev.model.utils import process\n\n# ✅ 비디오(mp4) 파일 경로\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\n\n# ✅ OpenCV를 사용한 비디오 로드\ncap = cv2.VideoCapture(video_path)\nframes = []\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nnum_frames = 16  # 초당 1프레임씩 총 16개 샘플링\n\n# ✅ 프레임 인덱스 선택\nindices = np.linspace(0, frame_count - 1, num_frames).astype(int)\n\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # OpenCV는 BGR → RGB 변환 필요\n    frames.append(frame)\n\ncap.release()  # 🔥 비디오 파일 닫기\n\n\n# 프레임을 Tensor로 변환: 최종 shape: (1, 3, num_frames, 224, 224)\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\n# --------------------------\n# 3. 비디오 입력 전처리 (process 함수 사용)\n# --------------------------\n# 여기서 'text'는 나중에 프롬프트와 함께 입력될 텍스트 부분을 위한 것입니다.\n# 하지만 우리는 직접 input_ids를 구성할 예정이므로 여기서는 비디오만 처리합니다.\ninputs_video = process(processor, video=frames_tensor)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# 4. 입력 시퀀스와 video_input_mask 생성 (EILeV 방식)\n# --------------------------\n# tokenizer 가져오기\ntokenizer = processor.tokenizer\n\n# 프롬프트 구성: 비디오 토큰 자리와 텍스트 프롬프트를 모두 포함\n# 예시 프롬프트: \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n\n# 필요한 토큰: bos_token, video placeholders (num_query_tokens), newline, prompt tokens\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 구성:\n#   [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\ninput_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids], dtype=torch.long).to(device)  # shape: (1, L)\nnum_tokens = input_ids.shape[1]\n\n# video_input_mask: True for video token positions, False for others.\n# bos 토큰: 0, video placeholders: 1, newline + prompt: 0\nvideo_input_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask], dtype=torch.bool).to(device)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# 5. Combine 입력 (비디오 픽셀 값)\n# --------------------------\n# inputs_video는 딕셔너리로, 특히 \"pixel_values\"가 포함됨.\ninputs = {}\ninputs[\"input_ids\"] = input_ids\ninputs[\"pixel_values\"] = inputs_video[\"pixel_values\"].to(device)\n\n# 디버깅 프린트\nprint(f\"🔍 input_ids.shape: {inputs['input_ids'].shape}\")      # 예: (1, L) where L = 1+32+1+len(prompt_tokens)\nprint(f\"🔍 video_input_mask.shape: {video_input_mask.shape}\")    # (1, L)\nprint(f\"🔍 video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\nprint(f\"🔍 pixel_values.shape: {inputs['pixel_values'].shape}\")    # (1, 3, num_frames, 224, 224)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ GPU 이동\ndevice = model.device\ninputs = {key: val.to(device) for key, val in inputs.items()}\nvideo_input_mask = video_input_mask.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ EILeV 모델 실행\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,  # 📌 필수 입력 추가\n        max_new_tokens=30,\n        num_beams=4,\n        repetition_penalty=1.5,\n    )\n\n# ✅ 생성된 텍스트 디코딩\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\n# ✅ 결과 출력\nprint(\"\\n🎬 Video Description Generated:\")\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ 기존 방식: 전체 프레임을 통합하여 하나의 설명 생성\nwith torch.no_grad():  # 🔥 그래디언트 계산 비활성화 (메모리 최적화)\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=pixel_values,  # 🔥 5D 텐서로 변환된 비디오 입력\n        video_input_mask=video_input_mask,  # ✅ 필수 추가\n        max_new_tokens=80,  # 🔥 설명 길이 조정\n        num_beams=4,  # 🔥 빔 서치 활용\n        repetition_penalty=1.3,  # 🔥 반복 줄이기\n    )\n\n# ✅ 생성된 텍스트 디코딩\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\nprint(\"\\n🎬 Video Description Generated:\")\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frame_paths = [\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0000.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0029.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0058.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0087.jpg\",\n    \"/kaggle/input/test-img-ive/Ady0zhVZn58_frame_0116.jpg\",\n]\nframes = [Image.open(path).convert(\"RGB\") for path in frame_paths]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ BLIP-2의 새로운 입력 처리 방식 적용\nprompt = \"Question: Describe in detail what actions are taking place in the video.\\nAnswer:\"\n\ninputs = processor(\n    images=frames,\n    text=[prompt] * len(frames),  # 🔥 모든 프레임에 같은 프롬프트 적용\n    return_tensors=\"pt\",\n    padding=True,  # 🔥 패딩 추가하여 크기 맞추기\n)\n\n# 🚀 Debugging: 변환된 입력 데이터 크기 확인\nprint(f\"Input IDs shape: {inputs['input_ids'].shape}\")  # (batch_size, sequence_length)\nprint(f\"Pixel Values shape: {inputs['pixel_values'].shape}\")  # (batch_size, 3, 224, 224)\n# ✅ GPU 이동\ninputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n\nprint(\"✅ 입력 데이터 변환 완료!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ 기존 방식: 전체 프레임을 통합하여 하나의 설명 생성\nwith torch.no_grad():  # 🔥 그래디언트 계산 비활성화 (메모리 최적화)\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=80,  # 길이 조정 유지\n        num_beams=4,  # 적절한 빔 서치 유지\n        repetition_penalty=1.3,  # 🔥 반복 문장 줄이기\n    )\n\n# ✅ 생성된 텍스트 디코딩\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##################################################################################","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\n\n# ==================================================\n# 1. 모델 및 프로세서 로드 (EILeV 전용)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\n# EILeV 전용 클래스로 로드 (8-bit, GPU 자동 배분)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n# 모델은 이미 올바른 디바이스에 로드됨 (8-bit 양자화 시 .to(device) 불필요)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. 비디오(mp4) 파일에서 프레임 추출 (OpenCV 사용)\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\nframes = []\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nnum_frames = 16  # 예: 초당 1프레임씩 총 16개 샘플링\n\n\n\n# TODO\n=====================================================\n초당 2 프레임씩 num_frames= 10 (5초) 짜리 frames 생성\n\ntotal_frames_list = []\n\n10_frames_list = 5초씩 자르기\n\nfor 24번 동안:\ntotal_frames_list.append(10_frames_list)\n\n\nresults = []\n#EILeV 모델 24번 호출:                         \nfor 10_frames_list in total_frames_list:\n    10_frames_list를 넣기\n    result = {}\n    blip_result = videoblipmodel(input_is, mask, )\n\n    result['time'] = time 정보\n    result['caption'] = blip_result\n    \n    results.append(result)\n\nresults 파일로 timeframe과 함께 파일로 쓰기\n=====================================================\n\n\n\nindices = np.linspace(0, frame_count - 1, num=num_frames).astype(int)\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    # OpenCV는 BGR → RGB 변환 필요\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frames.append(frame)\ncap.release()\n\n# 프레임을 Tensor로 변환 (최종 shape: [1, 3, num_frames, 224, 224])\n# (num_frames, H, W, C) → (1, C, num_frames, H, W)\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\n# ==================================================\n# 3. 비디오 입력 전처리 (process() 함수 사용)\n# ==================================================\n# 여기서는 비디오 부분만 처리하여 pixel_values를 생성합니다.\ninputs_video = process(processor, video=frames_tensor, text=None)\n# inputs_video[\"pixel_values\"]의 shape: (1, 3, num_frames, 224, 224)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 4. 입력 시퀀스 및 video_input_mask 구성 (EILeV 형식)\n# ==================================================\n# EILeV는 입력 시퀀스에 비디오 관련 자리(쿼리 토큰)를 먼저 배치한 후, 프롬프트 텍스트를 추가합니다.\nprompt = \"Question: Describe in detail what actions are taking place in the video.\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 입력 시퀀스 구성: [bos] + [pad] * num_query_tokens + [newline] + prompt_tokens\ninput_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids], dtype=torch.long).to(device)  # shape: (1, L)\nnum_tokens = input_ids.shape[1]\n\n# video_input_mask: bos 토큰 → 0, 비디오 자리 (쿼리 토큰) → 1, 나머지(텍스트) → 0\nvideo_input_mask_list = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n\n# ==================================================\n# 5. 최종 입력 구성\n# ==================================================\ninputs = {}\ninputs[\"input_ids\"] = input_ids\ninputs[\"pixel_values\"] = inputs_video[\"pixel_values\"].to(device)\n\n# ==================================================\n# 6. 디버깅 프린트\n# ==================================================\nprint(f\"🔍 input_ids.shape: {inputs['input_ids'].shape}\")          # 예: (1, L) where L = 1 + 32 + 1 + len(prompt_tokens)\nprint(f\"🔍 pixel_values.shape: {inputs['pixel_values'].shape}\")        # (1, 3, num_frames, 224, 224)\nprint(f\"🔍 video_input_mask.shape: {video_input_mask.shape}\")          # (1, L)\nprint(f\"🔍 video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\n\n# ==================================================\n# 7. 모델 실행 (EILeV)\n# ==================================================\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,  # 필수 입력: shape (1, L)\n        max_new_tokens=80,\n        num_beams=4,\n        repetition_penalty=1.3,\n    )\n\n# ==================================================\n# 8. 생성된 텍스트 디코딩 및 출력\n# ==================================================\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\nprint(\"\\n🎬 Video Description Generated:\")\nprint(generated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\n\n# ==================================================\n# 1. 모델 및 프로세서 로드 (EILeV 전용)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\n# EILeV 전용 모델 로드 (8-bit, GPU 자동 배분)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. 비디오 전체 정보 가져오기\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\n\n# ==================================================\n# 3. 세그먼트 분할 및 캡션 생성\n# ==================================================\nsegment_duration = 5  # 세그먼트 길이 (초)\nframes_per_segment = int(fps * segment_duration)\n\nmetadata = []  # 각 세그먼트의 메타데이터를 저장할 리스트\n\nfor start_frame in range(0, total_frames, frames_per_segment):\n    # 세그먼트의 시작과 끝 계산\n    end_frame = min(start_frame + frames_per_segment, total_frames)\n    num_segment_frames = end_frame - start_frame\n    \n    # 세그먼트 내에서 균등하게 프레임 선택\n    cap = cv2.VideoCapture(video_path)\n    indices = np.linspace(start_frame, end_frame - 1, num=num_segment_frames).astype(int)\n    segment_frames = []\n    for i in indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        segment_frames.append(frame)\n    cap.release()\n    \n    if len(segment_frames) == 0:\n        continue\n    \n    # -------------------------------\n    # 3-1. 프레임 전처리: Tensor 변환\n    # -------------------------------\n    # (num_frames, H, W, C) → (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    # -------------------------------\n    # 3-2. 비디오 전처리: process() 함수 사용\n    # -------------------------------\n    inputs_video = process(processor, video=segment_tensor, text=None)\n    \n    # -------------------------------\n    # 3-3. 입력 시퀀스 및 video_input_mask 구성\n    # -------------------------------\n    # 프롬프트 구성: 단일 Q&A 쌍\n    prompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\n    prompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n    \n    bos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n    pad_token = tokenizer.pad_token_id\n    newline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n    num_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n    \n    # 입력 시퀀스: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\n    input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\n    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n    num_tokens = input_ids.shape[1]\n    \n    # video_input_mask: bos → 0, 비디오 자리 (쿼리 토큰) → 1, 나머지 (텍스트) → 0\n    video_input_mask_list = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\n    video_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n    \n    # -------------------------------\n    # 3-4. 최종 입력 구성\n    # -------------------------------\n    inputs = {\n        \"input_ids\": input_ids,\n        \"pixel_values\": inputs_video[\"pixel_values\"].to(device)\n    }\n    \n    # 디버깅 프린트\n    print(f\"🔍 Segment starting at {start_frame/fps:.1f}s:\")\n    print(f\"   input_ids.shape: {inputs['input_ids'].shape}\")\n    print(f\"   pixel_values.shape: {inputs['pixel_values'].shape}\")\n    print(f\"   video_input_mask.shape: {video_input_mask.shape}\")\n    print(f\"   video_input_mask sum: {video_input_mask.sum().item()}\")\n    \n    # -------------------------------\n    # 3-5. 모델 실행 및 캡션 생성\n    # -------------------------------\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            video_input_mask=video_input_mask,\n            max_new_tokens=80,\n            num_beams=4,\n            repetition_penalty=1.3,\n        )\n    caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n    \n    # 세그먼트 시작 시간과 캡션 저장\n    metadata.append({\n        \"start_time\": start_frame / fps,\n        \"end_time\": end_frame / fps,\n        \"caption\": caption\n    })\n\n# ==================================================\n# 4. 결과 출력: 각 세그먼트별 캡션\n# ==================================================\nfor data in metadata:\n    print(f\"Segment {data['start_time']:.1f}s - {data['end_time']:.1f}s: {data['caption']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"===========================비디오 5초마다 설명생성===============================\nkpyu/eilev-blip2-opt-2.7b적용\n","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 1. 모델 및 프로세서 로드 (EILeV 전용)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EILeV 전용 모델 로드 (8-bit, GPU 자동 배분)\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n# 모델은 이미 올바른 디바이스에 로드됨 (8-bit 양자화 시 .to(device) 불필요)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 2. 비디오 전체 정보 가져오기\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\nprint(f\"Total frames: {total_frames}, FPS: {fps}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 3. 세그먼트 분할 및 캡션 생성\n# ==================================================\nsegment_duration = 5  # 세그먼트 길이 (초)\nframes_per_segment = int(fps * segment_duration)\n\n# 프롬프트 및 입력 시퀀스 구성 (모든 세그먼트에 동일)\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 입력 시퀀스: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\n# video_input_mask: bos 토큰 → 0, video 자리 (쿼리 토큰) → 1, 나머지 (텍스트) → 0\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# 결과 저장 리스트\nmetadata = []\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 4. 세그먼트별 캡션 생성\n# ==================================================\ncap = cv2.VideoCapture(video_path)\nfor start_frame in range(0, total_frames, frames_per_segment):\n    end_frame = min(start_frame + frames_per_segment, total_frames)\n    # 세그먼트 프레임 추출\n    segment_frames = []\n    for i in range(start_frame, end_frame):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # BGR → RGB\n        segment_frames.append(frame)\n    if len(segment_frames) == 0:\n        continue\n    \n    # 프레임을 Tensor로 변환: (num_frames, H, W, C) → (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    # EILeV 공식 process() 함수로 비디오 입력 전처리 (시간 정보 유지)\n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)  # (1, 3, num_frames, 224, 224)\n    \n    # 최종 입력 구성: 동일한 텍스트 입력 (base_input_ids)와 base_video_mask 사용\n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask  # shape: (1, num_tokens)\n    \n    # 디버깅 프린트 (각 세그먼트별)\n    print(f\"🔍 Segment starting at {start_frame/fps:.1f}s:\")\n    print(f\"   input_ids.shape: {inputs_seg['input_ids'].shape}\")    # (1, num_tokens)\n    print(f\"   pixel_values.shape: {inputs_seg['pixel_values'].shape}\")  # (1, 3, num_frames, 224, 224)\n    print(f\"   video_input_mask.shape: {video_input_mask_seg.shape}\")  # (1, num_tokens)\n    print(f\"   video_input_mask sum (should be {num_query_tokens}): {video_input_mask_seg.sum().item()}\")\n    # 모델 실행: 세그먼트별 캡션 생성\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=80,\n            num_beams=4,\n            repetition_penalty=1.3,\n        )\n    caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n    \n    # 세그먼트 시작 시간, 종료 시간, 캡션 저장\n    metadata.append({\n        \"start_time\": start_frame / fps,\n        \"end_time\": end_frame / fps,\n        \"caption\": caption\n    })\ncap.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['start_time']:.1f}s - {seg['end_time']:.1f}s: {seg['caption']}\")\n\n# Optional: 저장 (JSON 파일)\nwith open(\"video_captions.json\", \"w\") as f:\n    json.dump(metadata, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"===========================비디오 5초마다 설명생성=============================== kpyu/eilev-kpyu/eilev-blip2-flan-t5-xl적용","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 모델 및 프로세서 로드 (FLAN‑T5 기반 EILeV)\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. 비디오에서 프레임 추출 (예: 첫 5초, 초당 1프레임)\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nprint(f\"Total frames: {frame_count}, FPS: {fps}\")\nnum_frames = int(fps * 5)\nframes = []\nindices = np.linspace(0, num_frames - 1, num=num_frames).astype(int)\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frames.append(frame)\ncap.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. 프레임 전처리 (텐서 변환 및 재배열)\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\ninputs_video = process(processor, video=frames_tensor, text=None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. 입력 시퀀스 및 video_input_mask 구성 (학습 시 사용된 Q&A 형식 유지)\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\ninput_ids_list = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids_list], dtype=torch.long).to(device)\nnum_tokens = input_ids.shape[1]\nvideo_input_mask_list = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n\ninputs = {\n    \"input_ids\": input_ids,\n    \"pixel_values\": inputs_video[\"pixel_values\"].to(device)\n}\n\n# 디버깅: 입력 데이터 크기 확인\nprint(f\"🔍 input_ids.shape: {inputs['input_ids'].shape}\")         # (1, L)\nprint(f\"🔍 pixel_values.shape: {inputs['pixel_values'].shape}\")       # (1, 3, num_frames, 224, 224)\nprint(f\"🔍 video_input_mask.shape: {video_input_mask.shape}\")         # (1, L)\nprint(f\"🔍 video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. 모델 실행 및 캡션 생성 (디코딩 파라미터 조정)\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,\n        max_new_tokens=100,      # 더 긴 답변을 위해 늘림\n        num_beams=6,             # 빔 서치 증가\n        temperature=0.7,         # 온도 조절로 다양성 확보\n        repetition_penalty=1.1,  # 반복 억제 완화\n    )\n\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n# 후처리: 첫 번째 Q&A 쌍만 추출\nfinal_caption = generated_text.split(\"\\n\")[0]\n\nprint(\"\\n🎬 Video Description Generated:\")\nprint(final_caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['start_time']:.1f}s - {seg['end_time']:.1f}s: {seg['caption']}\")\n\n# Optional: 저장 (JSON 파일)\nwith open(\"video_captions(flan-t5).json\", \"w\") as f:\n    json.dump(metadata, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\n\n# ==================================================\n# 1. 모델 및 프로세서 로드 (FLAN‑T5 기반 EILeV)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # FLAN‑T5 기반 모델\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n)\n# 모델은 이미 적절한 디바이스에 로드됨 (추가 .to(device)는 필요없음)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. 비디오(mp4) 파일에서 프레임 추출 (예: 첫 5초, 초당 1프레임)\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nprint(f\"Total frames: {frame_count}, FPS: {fps}\")\nnum_frames = int(fps * 5)  # 5초 분량\nframes = []\nindices = np.linspace(0, num_frames - 1, num=num_frames).astype(int)\nfor i in indices:\n    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frames.append(frame)\ncap.release()\n\n# ==================================================\n# 3. 프레임 전처리: 텐서 변환 및 재배열 (최종 shape: [1, 3, num_frames, 224, 224])\n# ==================================================\nframes_tensor = torch.from_numpy(np.stack(frames)).permute(0, 3, 1, 2)  # (num_frames, 3, H, W)\nframes_tensor = frames_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\nframes_tensor = frames_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n\n# ==================================================\n# 4. 비디오 입력 전처리: process() 함수 사용\n# ==================================================\ninputs_video = process(processor, video=frames_tensor, text=None)\n\n# ==================================================\n# 5. 입력 시퀀스 및 video_input_mask 구성 (Q&A 형식 유지)\n# ==================================================\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32\n\n# 전체 입력 시퀀스 구성: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\ninput_ids_list = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\ninput_ids = torch.tensor([input_ids_list], dtype=torch.long).to(device)\nnum_tokens = input_ids.shape[1]\n\n# video_input_mask: [bos] → False, 다음 num_query_tokens → True, 나머지 → False\nvideo_input_mask_list = [False] + [True] * num_query_tokens + [False] * (num_tokens - (1 + num_query_tokens))\nvideo_input_mask = torch.tensor([video_input_mask_list], dtype=torch.bool).to(device)\n\n# ==================================================\n# 6. 최종 입력 구성\n# ==================================================\ninputs = {\n    \"input_ids\": input_ids,\n    \"pixel_values\": inputs_video[\"pixel_values\"].to(device)\n}\n\n# 디버깅: 입력 데이터 크기 확인\nprint(f\"🔍 input_ids.shape: {inputs['input_ids'].shape}\")         # 예: (1, L)\nprint(f\"🔍 pixel_values.shape: {inputs['pixel_values'].shape}\")       # (1, 3, num_frames, 224, 224)\nprint(f\"🔍 video_input_mask.shape: {video_input_mask.shape}\")         # (1, L)\nprint(f\"🔍 video_input_mask sum (should be {num_query_tokens}): {video_input_mask.sum().item()}\")\n\n# ==================================================\n# 7. 모델 실행 (캡션 생성) - 디코딩 파라미터 조정\n# ==================================================\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        video_input_mask=video_input_mask,\n        max_new_tokens=100,      # 더 긴 답변 생성\n        num_beams=6,             # 빔 서치 확대\n        temperature=0.7,         # 온도 조절\n        repetition_penalty=1.1,  # 반복 억제 완화\n    )\n\ngenerated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n\n# 후처리: 첫 번째 Q&A 쌍만 사용 (필요에 따라 수정)\nfinal_caption = generated_text.split(\"\\n\")[0]\n\nprint(\"\\n🎬 Video Description Generated:\")\nprint(final_caption)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"blip2-plant-t5 사용 EILEV 내부함수 사용\n","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n# 캐글 입력 경로: EILeV 저장소가 /kaggle/input/eilev-git/EILEV-main 에 있다고 가정\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom eilev.data.utils import generate_input_ids_and_labels_from_interleaved  # 내부 입력 구성 함수 (옵션)\nfrom PIL import Image\nimport json\nimport time\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 1. 모델 및 프로세서 로드 (EILeV, OPT 기반)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # 캐글에 업로드된 EILeV OPT 기반 체크포인트\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# CPU/GPU 환경에 따라 quantization 사용 여부 결정 (캐글에서는 보통 GPU 사용)\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\n# GPU 환경에서 8-bit 양자화된 모델은 이미 올바른 디바이스에 로드되므로 .to(device) 불필요\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 2. 비디오 전체 정보 가져오기\n# ==================================================\nvideo_path = \"/kaggle/input/testvideo-ive/Ady0zhVZn58.mp4\"\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\nprint(f\"Total frames: {total_frames}, FPS: {fps}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ==================================================\n# # 3. 기본 입력 시퀀스 및 video_input_mask 구성\n# #    (EILeV 학습 시 사용한 Q&A 형식 유지)\n# # ==================================================\n# prompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\n# prompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# bos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n# pad_token = tokenizer.pad_token_id\n# newline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n# num_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# # 전체 입력 시퀀스 구성: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\n# base_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\n# base_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\n# num_tokens = base_input_ids.shape[1]\n\n# # video_input_mask: bos 토큰 → 0, 다음 num_query_tokens (비디오 자리) → 1, 나머지 (텍스트 부분) → 0\n# base_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\n# base_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #opt 기반으로 설명생성 질문과 답은 후처리로 제거\n# # ==================================================\n# # 4. 세그먼트 분할 및 캡션 생성\n# # ==================================================\n# segment_duration = 5  # 세그먼트 길이 (초)\n# frames_per_segment = int(fps * segment_duration)\n# metadata = []  # 세그먼트별 메타데이터 저장 리스트\n\n# cap = cv2.VideoCapture(video_path)\n# current_segment = 0\n# start_time_overall = time.time()\n\n# while True:\n#     start_frame = current_segment * frames_per_segment\n#     if start_frame >= total_frames:\n#         break\n#     end_frame = min(start_frame + frames_per_segment, total_frames)\n#     segment_frames = []\n    \n#     # 세그먼트 내 프레임 추출\n#     for i in range(start_frame, end_frame):\n#         cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n#         ret, frame = cap.read()\n#         if not ret:\n#             break\n#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#         segment_frames.append(frame)\n    \n#     if len(segment_frames) == 0:\n#         break\n    \n#     # 프레임 텐서 변환: (num_frames, H, W, C) → (1, 3, num_frames, H, W)\n#     segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n#     segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n#     segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n#     # EILeV 공식 process() 함수 사용 (시간 정보 유지)\n#     inputs_video = process(processor, video=segment_tensor, text=None)\n#     pixel_values = inputs_video[\"pixel_values\"].to(device)  # (1, 3, num_segment_frames, 224, 224)\n    \n#     # 최종 입력 구성: 동일한 텍스트 입력과 video mask 사용\n#     inputs_seg = {\n#         \"input_ids\": base_input_ids,\n#         \"pixel_values\": pixel_values\n#     }\n#     video_input_mask_seg = base_video_mask  # (1, num_tokens)\n    \n#     seg_start_time = start_frame / fps\n#     seg_end_time = end_frame / fps\n#     print(f\"Segment {current_segment} ({seg_start_time:.1f}s - {seg_end_time:.1f}s):\")\n#     print(f\"   input_ids.shape: {inputs_seg['input_ids'].shape}\")    # (1, num_tokens)\n#     print(f\"   pixel_values.shape: {inputs_seg['pixel_values'].shape}\")  # (1, 3, num_segment_frames, 224, 224)\n#     print(f\"   video_input_mask.shape: {video_input_mask_seg.shape}\")  # (1, num_tokens)\n#     print(f\"   video_input_mask sum (should be {num_query_tokens}): {video_input_mask_seg.sum().item()}\")\n    \n#     # 모델 실행: 세그먼트별 캡션 생성\n#     with torch.no_grad():\n#         output = model.generate(\n#             input_ids=inputs_seg[\"input_ids\"],\n#             pixel_values=inputs_seg[\"pixel_values\"],\n#             video_input_mask=video_input_mask_seg,\n#             max_new_tokens=80,\n#             num_beams=4,\n#             repetition_penalty=1.3,\n#         )\n#     caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n#     print(f\"   Generated caption: {caption}\")\n    \n#     metadata.append({\n#         \"segment\": current_segment,\n#         \"start_time\": seg_start_time,\n#         \"end_time\": seg_end_time,\n#         \"caption\": caption\n#     })\n#     current_segment += 1\n\n# cap.release()\n# total_time = time.time() - start_time_overall\n# print(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n# with open(\"/kaggle/working/video_captions_opt.json\", \"w\", encoding=\"utf-8\") as f:\n#     json.dump(metadata, f, indent=4, ensure_ascii=False)\n# print(\"Result saved to 'output/video_captions_opt.json'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 3. 세그먼트 분할 및 캡션 생성\n# ==================================================\nsegment_duration = 5  # 세그먼트 길이 (초)\nframes_per_segment = int(fps * segment_duration)\nmetadata = []  # 각 세그먼트 메타데이터 저장 리스트\n\n# FLAN‑T5에 맞게 프롬프트 변경: 단일 서술형 프롬프트 사용\n# (Q&A 형식이 아니라 전체 설명 서술을 유도)\nprompt = \"Describe in detail what actions are taking place in the video.\"\n# prompt_tokens는 내부 함수에서 자동 처리되도록 텍스트 입력을 None으로 처리합니다.\n# (따라서 텍스트 프롬프트는 process() 함수 대신 별도로 input_ids를 구성하지 않습니다.)\n\n# 대신, EILeV 내부 함수에서 비디오 부분만 처리하도록 합니다.\n# 아래 base_input_ids와 video_input_mask는 EILeV 학습 시 사용한 형식으로 유지합니다.\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 기본 입력 시퀀스 구성: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\n# 여기서는 prompt_tokens 대신 FLAN‑T5에 맞게 간단한 서술형 프롬프트를 사용할 수 있습니다.\n# 예를 들어, 프롬프트에 \"Describe in detail what is happening in the video.\" 만 사용합니다.\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token]\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\n# video_input_mask: bos 토큰 → 0, 다음 num_query_tokens (비디오 자리) → 1, 나머지는 0\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# 4. 세그먼트 분할 및 캡션 생성 (오버랩 적용)\n# ==================================================\n# 세그먼트 길이 및 오버랩 설정 (초 단위)\nsegment_duration = 5          # 전체 세그먼트 길이 (초)\noverlap_duration = 2          # 오버랩 길이 (초)\n\nframes_per_segment = int(fps * segment_duration)\noverlap_frames = int(fps * overlap_duration)\n# 각 세그먼트 시작 프레임: 세그먼트 전체 길이에서 오버랩 부분을 뺀 값만큼씩 진행\nstep_frames = frames_per_segment - overlap_frames\n\nmetadata = []  # 각 세그먼트의 메타데이터 저장 리스트\ncurrent_segment = 0\nstart_time_overall = time.time()\n\ncap = cv2.VideoCapture(video_path)\nwhile True:\n    start_frame = current_segment * step_frames\n    if start_frame >= total_frames:\n        break\n    # 각 세그먼트는 start_frame부터 frames_per_segment 만큼의 프레임을 포함\n    end_frame = min(start_frame + frames_per_segment, total_frames)\n    \n    # 세그먼트에 해당하는 프레임 추출\n    segment_frames = []\n    for i in range(start_frame, end_frame):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        segment_frames.append(frame)\n    \n    if len(segment_frames) == 0:\n        break\n\n    # 프레임을 텐서로 변환: (num_frames, H, W, C) → (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    # EILeV 공식 process() 함수 사용 (text 입력은 None으로 설정)\n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    # 최종 입력 구성: base_input_ids와 base_video_mask 사용\n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    seg_start_time = start_frame / fps\n    seg_end_time = end_frame / fps\n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    print(f\"   input_ids.shape: {inputs_seg['input_ids'].shape}\")\n    print(f\"   pixel_values.shape: {inputs_seg['pixel_values'].shape}\")\n    print(f\"   video_input_mask.shape: {video_input_mask_seg.shape}\")\n    print(f\"   video_input_mask sum (should be {num_query_tokens}): {video_input_mask_seg.sum().item()}\")\n    \n    # 모델 실행: 캡션 생성\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,   # max_new_tokens 값을 늘려서 더 긴 캡션 생성\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    \n    current_segment += 1\n\ncap.release()\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:15:34.443132Z","iopub.execute_input":"2025-03-18T03:15:34.443455Z","iopub.status.idle":"2025-03-18T03:15:58.159689Z","shell.execute_reply.started":"2025-03-18T03:15:34.443428Z","shell.execute_reply":"2025-03-18T03:15:58.158449Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nCollecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=ccbe3d561d2756f7eecce89db960d19c225ac9ccb301521c8ddc9fbd09fe49b0\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=5b77018f12e696091fce374801958c847b417a8ca01969a8126539dcfe7e3117\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=de281f6542ee125aaa2b3bcaa56ca3a3b717ea134276d4275a5ede9dea3702b8\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.2.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\nCollecting decord\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: decord\nSuccessfully installed decord-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\n# 캐글 입력 경로: EILeV 저장소가 /kaggle/input/eilev-git/EILEV-main 에 있다고 가정\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\n\n\nfrom collections import deque  # 슬라이딩 윈도우를 위해 필요합니다.\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:20:57.064203Z","iopub.execute_input":"2025-03-18T03:20:57.064648Z","iopub.status.idle":"2025-03-18T03:20:57.069832Z","shell.execute_reply.started":"2025-03-18T03:20:57.064609Z","shell.execute_reply":"2025-03-18T03:20:57.068831Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# ==================================================\n# 1. 모델 및 프로세서 로드 (EILeV, OPT 기반)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # 캐글에 업로드된 EILeV OPT 기반 체크포인트\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:16:26.802496Z","iopub.execute_input":"2025-03-18T03:16:26.803101Z","iopub.status.idle":"2025-03-18T03:19:04.288576Z","shell.execute_reply.started":"2025-03-18T03:16:26.803077Z","shell.execute_reply":"2025-03-18T03:19:04.287905Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a191efc242ff4b5b88b5607056ba9042"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc203428aca1451a9c109ce18c29b78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27418104e85d4380bbfbd79b9103cb63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4991442af5ab40f1968ff11ad7ecdd60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc5f2d0065440b29c22075da370a95a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1184664c5c784250bb2ffa6e4b126ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c0a53ff806349ddbe71b175227d44ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee055a9a71d4021813722c92725364e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f01d16532401497cbf9e010b34b62e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f181c60f1da4b8882a1d6cacf82aace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ade9c29d8c44a7f8d22ec438e93189f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa4b077f0b14a329bae0a75b1299fd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b460d01666cc4a0fb6a6f0490e0643e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57b8da0114249d981e5d53fb99fdbeb"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ==================================================\n# 2. 영상 읽기 (imageio를 스트리밍 방식으로 사용)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # 새 영상 경로\n\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    fps = meta['fps']\n    print(f\"Imageio - FPS: {fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# ==================================================\n# 3. 기본 입력 구성 (FLAN‑T5 단일 서술형 프롬프트 방식)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 추가할 프롬프트 텍스트 정의 (원하는 설명을 유도할 문구)\nprompt = \"Describe in detail what actions are taking place in the video, including dynamic movements like walking, running, or interacting with objects.\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# 기본 입력 시퀀스 구성: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 4. 슬라이딩 윈도우를 이용한 세그먼트 분할 및 캡션 생성\n# ==================================================\nsegment_duration = 5    # 세그먼트 길이 (초)\noverlap_duration = 2    # 오버랩 길이 (초)\n\nframes_per_segment = int(fps * segment_duration)\noverlap_frames = int(fps * overlap_duration)\nstep_frames = frames_per_segment - overlap_frames\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# deque를 사용하여 sliding window 구성 (최대 frames_per_segment 개의 프레임 유지)\nwindow = deque(maxlen=frames_per_segment)\nframe_idx = 0\n\n# 초기 윈도우 채우기\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i)\n        window.append(frame)\n        frame_idx = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # 첫 세그먼트 처리 (프레임 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / fps\n    segment_frames = list(window)\n    \n    # 프레임 텐서 변환: (num_frames, H, W, C) → (1, 3, num_frames, H, W)\n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    current_segment += 1\n\n    # 이후 슬라이딩 윈도우로 세그먼트 처리\n    while True:\n        count = 0\n        # step_frames 만큼 새로운 프레임 추가 (윈도우는 자동으로 오래된 프레임을 제거)\n        while count < step_frames:\n            try:\n                frame = reader.get_data(frame_idx)\n                window.append(frame)\n                frame_idx += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (frame_idx - frames_per_segment) / fps\n        seg_end_time = frame_idx / fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        caption = generated_text.strip()\n        print(f\"   Generated caption: {caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:31:45.615095Z","iopub.execute_input":"2025-03-18T03:31:45.615447Z","iopub.status.idle":"2025-03-18T03:34:21.107507Z","shell.execute_reply.started":"2025-03-18T03:31:45.615422Z","shell.execute_reply":"2025-03-18T03:34:21.106181Z"}},"outputs":[{"name":"stdout","text":"Imageio - FPS: 29.97\nframes_per_segment: 149, step_frames: 90\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 5.0s\n   Generated caption: ji yoon-ah and ji kyung-hee in a park\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 3.0s - 8.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 6.0s - 11.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 9.0s - 14.0s\n   Generated caption: ji yeon-ah and ji yeon-soo are playing a game of go-go on the grass.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 12.0s - 17.0s\n   Generated caption: ji yoon-ah and ji yoon-ah are seen playing on a kick scooter in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 5: 15.0s - 20.0s\n   Generated caption: ji yeon-ah and ji yeon-soo in\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 6: 18.0s - 23.0s\n   Generated caption: ji yoon-ah and ji kyung-hee have a romantic dinner together\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-12f56433efd8>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msegment_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0minputs_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_video\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/eilev-git/EILEV-main/eilev/model/utils.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(processor, video, text)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip_2/processing_blip_2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mimage_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moutput_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m                 )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvalid_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/image_processing_blip.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             images = [\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/image_processing_blip.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             images = [\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/image_processing_blip.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         return resize(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;31m# PIL images are in the format (width, height)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducing_gap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreducing_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 )\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m     def reduce(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg\n\nimport sys\nfrom collections import deque\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio\n\n# ==================================================\n# 1. 모델 및 프로세서 로드 (EILEV, OPT 기반)\n# ==================================================\n# 아래 모델 이름은 예시입니다. 실제 사용하시는 OPT 기반 모델 이름으로 수정하세요.\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"  \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n\n# ==================================================\n# 2. 영상 읽기 (imageio를 스트리밍 방식으로 사용)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # 영상 경로 (원하는 영상으로 변경)\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    fps = meta['fps']\n    print(f\"Imageio - FPS: {fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# ==================================================\n# 3. 기본 입력 구성 (OPT 기반 EILEV를 위한 단일 서술형 프롬프트)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 추가 프롬프트: 동적 행동을 보다 구체적으로 설명하도록 유도\nprompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# 기본 입력 시퀀스 구성: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 4. 슬라이딩 윈도우를 이용한 세그먼트 분할 및 캡션 생성 (오버랩 적용)\n# ==================================================\nsegment_duration = 5    # 세그먼트 길이 (초)\noverlap_duration = 2    # 오버랩 길이 (초)\n\nframes_per_segment = int(fps * segment_duration)\noverlap_frames = int(fps * overlap_duration)\nstep_frames = frames_per_segment - overlap_frames\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# deque를 사용하여 sliding window 구성 (최대 frames_per_segment 개의 프레임 유지)\nwindow = deque(maxlen=frames_per_segment)\nframe_idx = 0\n\n# 초기 윈도우 채우기\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i)\n        window.append(frame)\n        frame_idx = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # 첫 세그먼트 처리 (프레임 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / fps\n    segment_frames = list(window)\n    \n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    current_segment += 1\n\n    # 이후 슬라이딩 윈도우로 세그먼트 처리\n    while True:\n        count = 0\n        while count < step_frames:\n            try:\n                frame = reader.get_data(frame_idx)\n                window.append(frame)\n                frame_idx += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (frame_idx - frames_per_segment) / fps\n        seg_end_time = frame_idx / fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        caption = generated_text.strip()\n        print(f\"   Generated caption: {caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T03:50:10.711110Z","iopub.execute_input":"2025-03-18T03:50:10.711501Z","iopub.status.idle":"2025-03-18T03:53:45.866124Z","shell.execute_reply.started":"2025-03-18T03:50:10.711471Z","shell.execute_reply":"2025-03-18T03:53:45.864517Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: pytorchvideo in /usr/local/lib/python3.10/dist-packages (0.1.5)\nRequirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.5.post20221221)\nRequirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (14.2.0)\nRequirement already satisfied: parameterized in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.9.0)\nRequirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.10)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nRequirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.1.8)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9137ca2bcf48c5b2f1459a487cb9b7"}},"metadata":{}},{"name":"stdout","text":"Imageio - FPS: 29.97\nframes_per_segment: 149, step_frames: 90\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 5.0s\n   Generated caption: A man and a woman are walking on a bridge. The man is wearing a baseball cap and the woman is wearing a hat. The man is talking to the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the man. The man is looking at the woman and the woman is looking at the\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 3.0s - 8.0s\n   Generated caption: A man and a woman are playing a game of frisbee. The man throws the frisbee to the woman. The woman catches the frisbee and throws it to the man. The man catches the frisbee and throws it to the woman. The woman catches the frisbee and throws it to the man. The man catches the frisbee and throws it to the woman. The woman catches the frisbee and throws it to the man. The man catches the frisbee and throws it to the woman. The woman catches the frisbee and throws it to\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 6.0s - 11.0s\n   Generated caption: A group of people are playing a game of kite flying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 9.0s - 14.0s\n   Generated caption: A group of people are playing a game of kick scooter.\nExplanation: A group of people are playing a game of kick scooter.\nCategories\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 12.0s - 17.0s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-96359562dddb>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             output = model.generate(\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/eilev-git/EILEV-main/eilev/model/v2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, pixel_values, video_input_mask, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_input_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         outputs = self.language_model.generate(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2283\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2284\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids)\u001b[0m\n\u001b[1;32m    931\u001b[0m                 )\n\u001b[1;32m    932\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    934\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache, position_ids)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, position_ids)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# reuse k, v, self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fp16_weights\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul8bitLt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# Fast path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8_vectorwise_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mCAt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCAt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mint8_vectorwise_quant\u001b[0;34m(A, threshold)\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0moutlier_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_cuda_device_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:22:36.143765Z","iopub.execute_input":"2025-03-18T05:22:36.143968Z","iopub.status.idle":"2025-03-18T05:22:58.839108Z","shell.execute_reply.started":"2025-03-18T05:22:36.143948Z","shell.execute_reply":"2025-03-18T05:22:58.837999Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nCollecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=4856419f5ca7955b4b67b80861535e76ff0595b240a894a5f62197c13c05efb7\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=9cf4035d15573696a037fe8e0932847285e780c26112ab72b83aec2151915551\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=8308ef9e38a2b6303fab924cdd6b220244cdc4cf78d980ef863accfaa02600e3\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.2.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\nCollecting decord\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: decord\nSuccessfully installed decord-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport sys\n# 캐글 입력 경로: EILeV 저장소가 /kaggle/input/eilev-git/EILEV-main 에 있다고 가정\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\nfrom collections import deque\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:24:09.066698Z","iopub.execute_input":"2025-03-18T05:24:09.066992Z","iopub.status.idle":"2025-03-18T05:24:09.245468Z","shell.execute_reply.started":"2025-03-18T05:24:09.066971Z","shell.execute_reply":"2025-03-18T05:24:09.244571Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==================================================\n# 1. 모델 및 프로세서 로드 (FLAN‑T5 기반 EILEV)\n# ==================================================\nmodel_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # 사용하시는 체크포인트에 맞게 수정\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:24:11.592869Z","iopub.execute_input":"2025-03-18T05:24:11.593195Z","iopub.status.idle":"2025-03-18T05:26:34.712658Z","shell.execute_reply.started":"2025-03-18T05:24:11.593166Z","shell.execute_reply":"2025-03-18T05:26:34.711693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9c81181db4549a19e3f84edd78bbfba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85cff720aaea4ceab8dcd001f0863ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed06e78dbe0e4042bac96a1810355ba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a49bf9decd64416a12ff230e8eeec38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f169a436354e00a4077b33a6eed6d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543b72e113f247fa8658c68c75fcea8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a58df450f594fa289bef1493b4f583f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da0282d6d9a04b66adb54a5157ebd1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0106445cbb5472c9ab51fb3ce46c5e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb2c63be77f4472ae7cd1edc0f92a37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73aec6cda3a849bca3f6369cf3109d2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d64e930b71b4898adf51fb873013e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541e761077c248ccbd94a3d7f5ede43b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fce58f95ab84c3b916384f63cef0c54"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ==================================================\n# 2. 영상 읽기 (imageio를 스트리밍 방식으로 사용)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # 새 영상 경로\n\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    original_fps = meta['fps']\n    print(f\"Imageio - Original FPS: {original_fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# 원하는 샘플링 설정: 초당 5프레임\ndesired_fps = 5  \n# 원본 fps에서 몇 프레임마다 샘플링할지 계산 (예: 29.97/5 ≈ 6)\nsampling_interval = int(round(original_fps / desired_fps))\nprint(f\"Sampling interval (in original frames): {sampling_interval}\")\n\n# ==================================================\n# 3. 기본 입력 구성 (EILEV가 학습시킨 프롬프트 사용)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼리 토큰 개수\n\n# 기존 EILEV 프롬프트\nprompt = \"Describe in detail what actions are taking place in the video.\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# 기본 입력 시퀀스 구성: [bos] + [pad]*num_query_tokens + [newline] + prompt_tokens\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 4. 슬라이딩 윈도우를 이용한 세그먼트 분할 및 캡션 생성\n# ==================================================\n# 세그먼트 설정: 세그먼트 길이를 6초, 오버랩은 2초로 설정\nsegment_duration = 6    # 초 단위\noverlap_duration = 2    # 초 단위\n\nframes_per_segment = int(desired_fps * segment_duration)  # 예: 5 fps * 6초 = 30 프레임\noverlap_frames = int(desired_fps * overlap_duration)        # 예: 5 fps * 2초 = 10 프레임\nstep_frames = frames_per_segment - overlap_frames           # 예: 30 - 10 = 20 프레임\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# deque를 사용하여 sliding window 구성 (최대 frames_per_segment 개의 프레임 유지)\nwindow = deque(maxlen=frames_per_segment)\nsample_index = 0  # 샘플 단위 인덱스 (실제 프레임 번호는 sample_index * sampling_interval)\n\n# 초기 윈도우 채우기 (desired_fps 기반 샘플링)\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i * sampling_interval)\n        window.append(frame)\n        sample_index = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # 첫 세그먼트 처리 (샘플 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / desired_fps  # 예: 30 / 5 = 6초\n    segment_frames = list(window)\n    \n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    caption = generated_text.strip()\n    print(f\"   Generated caption: {caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": caption\n    })\n    current_segment += 1\n\n    # 이후 슬라이딩 윈도우로 세그먼트 처리\n    while True:\n        count = 0\n        # step_frames 만큼 새로운 프레임 추가 (윈도우는 자동으로 오래된 프레임 제거)\n        while count < step_frames:\n            try:\n                frame = reader.get_data(sample_index * sampling_interval)\n                window.append(frame)\n                sample_index += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (sample_index - frames_per_segment) / desired_fps\n        seg_end_time = sample_index / desired_fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        caption = generated_text.strip()\n        print(f\"   Generated caption: {caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_flan_t5.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_flan_t5.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T05:58:49.167094Z","iopub.execute_input":"2025-03-18T05:58:49.167441Z","iopub.status.idle":"2025-03-18T06:00:49.433304Z","shell.execute_reply.started":"2025-03-18T05:58:49.167414Z","shell.execute_reply":"2025-03-18T06:00:49.431887Z"}},"outputs":[{"name":"stdout","text":"Imageio - Original FPS: 29.97\nSampling interval (in original frames): 6\nframes_per_segment: 30, step_frames: 20\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 6.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 4.0s - 10.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 8.0s - 14.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 12.0s - 18.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 16.0s - 22.0s\n   Generated caption: \n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 5: 20.0s - 26.0s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a76205f07724>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             output = model.generate(\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/eilev-git/EILEV-main/eilev/model/v2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, pixel_values, video_input_mask, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_input_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         outputs = self.language_model.generate(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2283\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2284\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     ):\n\u001b[0;32m--> 675\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"!pip install -U bitsandbytes \n!pip install pytorchvideo \n!pip install decord imageio imageio-ffmpeg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:03:56.031274Z","iopub.execute_input":"2025-03-18T10:03:56.031654Z","iopub.status.idle":"2025-03-18T10:04:18.296292Z","shell.execute_reply.started":"2025-03-18T10:03:56.031623Z","shell.execute_reply":"2025-03-18T10:04:18.295478Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nCollecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.5.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (11.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=9cf53af577c20bad6874380da60bd527dd044ffa355bf51959a597c7c82d0c81\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=4d2e8ad582a9606ac94ada9ea88bad8445124fccf0e68432337f7bc7715d1858\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=aa9f95a6f30626cf7ac24b8bf159f8985c301b4f5ea65d165b984f6f39a51c92\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.2.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\nCollecting decord\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: decord\nSuccessfully installed decord-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\n# 캐글 입력 경로: EILeV 저장소가 /kaggle/input/eilev-git/EILEV-main 에 있다고 가정\nsys.path.append(\"/kaggle/input/eilev-git/EILEV-main\")\nfrom collections import deque\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig\nfrom eilev.model.v2 import VideoBlipForConditionalGeneration\nfrom eilev.model.utils import process\nfrom PIL import Image\nimport json\nimport time\nimport re\nimport imageio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:04:38.458948Z","iopub.execute_input":"2025-03-18T10:04:38.459517Z","iopub.status.idle":"2025-03-18T10:04:38.463807Z","shell.execute_reply.started":"2025-03-18T10:04:38.459494Z","shell.execute_reply":"2025-03-18T10:04:38.462983Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==================================================\n# 1. 모델 및 프로세서 로드 (EILEV, OPT 기반)\n# ==================================================\n# 실제 사용하시는 OPT 기반 모델 이름으로 수정하세요.\nmodel_name = \"kpyu/eilev-blip2-opt-2.7b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\nmodel = VideoBlipForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=quant_config,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\ntokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:19:28.978778Z","iopub.execute_input":"2025-03-18T06:19:28.979193Z","iopub.status.idle":"2025-03-18T06:24:36.639073Z","shell.execute_reply.started":"2025-03-18T06:19:28.979156Z","shell.execute_reply":"2025-03-18T06:24:36.638332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abe77ebe14f4d6b8ae76201813d9ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/122k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b832ead706564ef5b93201d86d7a54d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b391aa1078e849cb9510535f32087a15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292fdb63a52c4f67a9b1aa4c65fdfc24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a25ddcf74946dd9c6731a896aaaa55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f1dd7a409c43acb6afd7e7c29a4779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/105M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d4f17abf3e4f3c9f9cd641a270bbfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d05a36b1f748348028202ae7daf4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a5f63b161349d9b9b04bca4b192739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4d250875044165a010560385c0c8a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"336889d8868d4e4ab93c778aeff9e0d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79a4b4984964deda0f0240ea0eb1077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8f18bb2c2f49ed941bb546d80307b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d5481bf044f48b5839779d29f5f0397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e84632bb0f402191e0fbd097ea4768"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ==================================================\n# 2. 영상 읽기 (imageio를 스트리밍 방식으로 사용)\n# ==================================================\nvideo_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # 영상 경로\n\ntry:\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    meta = reader.get_meta_data()\n    original_fps = meta['fps']\n    print(f\"Imageio - Original FPS: {original_fps}\")\nexcept Exception as e:\n    raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# 원하는 샘플링 설정: 초당 15프레임\ndesired_fps = 15  \nsampling_interval = int(round(original_fps / desired_fps))\nprint(f\"Sampling interval (in original frames): {sampling_interval}\")\n\n# ==================================================\n# 3. 기본 입력 구성 (OPT 기반 EILEV 프롬프트)\n# ==================================================\nbos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\npad_token = tokenizer.pad_token_id\nnewline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\nnum_query_tokens = 32  # 영상 관련 쿼스트 토큰 개수\n\n# 기존 프롬프트: 학습 시 사용했던 형식\nprompt = \" Describe in detail what actions are taking place in the video.:\"\nprompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\nbase_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\nbase_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\nnum_tokens = base_input_ids.shape[1]\n\nbase_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\nbase_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# ==================================================\n# 후처리 함수: 불필요한 접두어 제거\n# ==================================================\ndef clean_caption(text):\n    # 각 줄마다 \"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\" 등 접두어 제거\n    lines = text.splitlines()\n    filtered = [line.strip() for line in lines if not any(line.strip().startswith(prefix) for prefix in \n                                                           [\"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\"])]\n    return \" \".join(filtered).strip()\n\n# ==================================================\n# 후처리 함수: 중복 문장 제거\n# ==================================================\ndef remove_duplicate_sentences(text):\n    # 문장을 마침표를 기준으로 분리\n    sentences = re.split(r'\\.\\s*', text)\n    seen = set()\n    unique_sentences = []\n    for s in sentences:\n        s = s.strip()\n        if s and s not in seen:\n            seen.add(s)\n            unique_sentences.append(s)\n    # 중복 제거한 문장을 다시 마침표로 이어 붙임\n    result = '. '.join(unique_sentences)\n    if result and not result.endswith('.'):\n        result += '.'\n    return result\n\n# ==================================================\n# 4. 슬라이딩 윈도우를 이용한 세그먼트 분할 및 캡션 생성 (오버랩 적용)\n# ==================================================\n# 세그먼트 설정: 세그먼트 길이를 5초, 오버랩은 1초로 설정\nsegment_duration = 5    # 초 단위\noverlap_duration = 1    # 초 단위\n\nframes_per_segment = int(desired_fps * segment_duration)   # 15 fps * 5초 = 75 프레임\noverlap_frames = int(desired_fps * overlap_duration)         # 15 fps * 1초 = 15 프레임\nstep_frames = frames_per_segment - overlap_frames            # 75 - 15 = 60 프레임\n\nprint(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\nmetadata = []\ncurrent_segment = 0\nstart_time_overall = time.time()\n\n# deque를 사용하여 슬라이딩 윈도우 구성 (최대 frames_per_segment 개의 프레임 유지)\nwindow = deque(maxlen=frames_per_segment)\nsample_index = 0  # 실제 프레임 번호 = sample_index * sampling_interval\n\n# 초기 윈도우 채우기 (desired_fps 기반 샘플링)\nprint(\"Preloading initial window...\")\nfor i in range(frames_per_segment):\n    try:\n        frame = reader.get_data(i * sampling_interval)\n        window.append(frame)\n        sample_index = i + 1\n    except Exception:\n        break\n\nif len(window) < frames_per_segment:\n    print(\"Not enough frames to form a full segment. Exiting.\")\nelse:\n    # 첫 세그먼트 처리 (샘플 인덱스 0 ~ frames_per_segment-1)\n    seg_start_time = 0.0\n    seg_end_time = frames_per_segment / desired_fps  # 예: 30 / 6 = 5초\n    segment_frames = list(window)\n    \n    segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n    segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n    segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n    inputs_video = process(processor, video=segment_tensor, text=None)\n    pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n    inputs_seg = {\n        \"input_ids\": base_input_ids,\n        \"pixel_values\": pixel_values\n    }\n    video_input_mask_seg = base_video_mask\n    \n    print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=inputs_seg[\"input_ids\"],\n            pixel_values=inputs_seg[\"pixel_values\"],\n            video_input_mask=video_input_mask_seg,\n            max_new_tokens=120,\n            num_beams=6,\n            repetition_penalty=1.2,\n        )\n    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n    # 후처리: 불필요한 접두어 제거 후, 중복 문장 제거\n    final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n    print(f\"   Generated caption: {final_caption}\")\n    \n    metadata.append({\n        \"segment\": current_segment,\n        \"start_time\": seg_start_time,\n        \"end_time\": seg_end_time,\n        \"caption\": final_caption\n    })\n    current_segment += 1\n\n    # 이후 슬라이딩 윈도우로 세그먼트 처리\n    while True:\n        count = 0\n        while count < step_frames:\n            try:\n                frame = reader.get_data(sample_index * sampling_interval)\n                window.append(frame)\n                sample_index += 1\n                count += 1\n            except Exception:\n                break\n        if count < step_frames:\n            print(\"Reached end of video during sliding window update.\")\n            break\n        \n        seg_start_time = (sample_index - frames_per_segment) / desired_fps\n        seg_end_time = sample_index / desired_fps\n        segment_frames = list(window)\n        \n        segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n        segment_tensor = segment_tensor.unsqueeze(0)\n        segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n        inputs_video = process(processor, video=segment_tensor, text=None)\n        pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n        inputs_seg = {\n            \"input_ids\": base_input_ids,\n            \"pixel_values\": pixel_values\n        }\n        video_input_mask_seg = base_video_mask\n        \n        print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n        with torch.no_grad():\n            output = model.generate(\n                input_ids=inputs_seg[\"input_ids\"],\n                pixel_values=inputs_seg[\"pixel_values\"],\n                video_input_mask=video_input_mask_seg,\n                max_new_tokens=120,\n                num_beams=6,\n                repetition_penalty=1.2,\n            )\n        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n        final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n        print(f\"   Generated caption: {final_caption}\")\n        \n        metadata.append({\n            \"segment\": current_segment,\n            \"start_time\": seg_start_time,\n            \"end_time\": seg_end_time,\n            \"caption\": final_caption\n        })\n        current_segment += 1\n\ntotal_time = time.time() - start_time_overall\nprint(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# ==================================================\n# 5. 결과 출력 및 파일 저장\n# ==================================================\nfor seg in metadata:\n    print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\nwith open(\"/kaggle/working/video_captions_opt.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, indent=4, ensure_ascii=False)\nprint(\"Result saved to '/kaggle/working/video_captions_opt.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T06:54:16.976047Z","iopub.execute_input":"2025-03-18T06:54:16.976412Z","iopub.status.idle":"2025-03-18T08:32:14.531947Z","shell.execute_reply.started":"2025-03-18T06:54:16.976379Z","shell.execute_reply":"2025-03-18T08:32:14.530787Z"}},"outputs":[{"name":"stdout","text":"Imageio - Original FPS: 29.97\nSampling interval (in original frames): 2\nframes_per_segment: 75, step_frames: 60\nPreloading initial window...\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 0: 0.0s - 5.0s\n   Generated caption: A man and a woman are walking on a bridge. The man is wearing a baseball cap and the woman is wearing a hat. The man is talking to the woman. The woman is looking at the man. The man is looking at the woman. The woman is looking at the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 1: 4.0s - 9.0s\n   Generated caption: A group of people are playing a game of kite flying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 2: 8.0s - 13.0s\n   Generated caption: A group of people are playing a game of kick the can.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 3: 12.0s - 17.0s\n   Generated caption: A group of people are riding a scooter. Categories.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 4: 16.0s - 21.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a hat. The man is holding a fishing rod and the woman is holding a fishing pole. The man is looking at the woman and the woman is looking at the man. The woman is smiling and the man is smiling. The man is looking at the woman and the woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 5: 20.0s - 25.0s\n   Generated caption: A group of people are sitting around a table. They are eating some food. One of the people is holding a chopstick. The other person is holding a bowl of food. The third person is holding a bowl of food. The fourth person is holding a bowl of food. The fifth person is holding a bowl of food. The sixth person is holding a bowl of food. The seventh person is holding a bowl of food. The eighth person is holding a bowl of food. The ninth person is holding a bowl of food. The tenth person.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 6: 24.0s - 29.0s\n   Generated caption: A group of people are sitting around a fire pit. One of the people is using a cell phone. The other people are talking to each other. The other people.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 7: 28.0s - 33.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at his phone. The woman is looking at her phone.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 8: 32.0s - 37.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 9: 36.0s - 41.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 10: 40.0s - 45.0s\n   Generated caption: In the video, there is a man and a woman sitting in chairs. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 11: 44.0s - 49.0s\n   Generated caption: In the video, there are three people sitting in a lawn chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 12: 48.0s - 53.0s\n   Generated caption: The image shows a stream of water flowing through a forest.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 13: 52.0s - 57.0s\n   Generated caption: The image shows a stream of water flowing through a grassy area. Other: The image shows a stream of water flowing through a grassy area.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 14: 56.0s - 61.0s\n   Generated caption: The picture shows a group of people crossing a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 15: 60.0s - 65.0s\n   Generated caption: A man and a woman are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 16: 64.0s - 69.0s\n   Generated caption: A man and a woman are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 17: 68.0s - 73.0s\n   Generated caption: A group of people are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 18: 72.0s - 77.0s\n   Generated caption: A man and a woman are walking on a bridge. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 19: 76.0s - 81.0s\n   Generated caption: A man and a woman are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 20: 80.0s - 85.0s\n   Generated caption: A group of people are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 21: 84.0s - 89.0s\n   Generated caption: A group of people are walking on a bridge.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 22: 88.0s - 93.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 23: 92.0s - 97.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 24: 96.0s - 101.0s\n   Generated caption: A man is talking to a woman. Woman: A man is talking to a woman. Camer.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 25: 100.0s - 105.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 26: 104.0s - 109.0s\n   Generated caption: In the video, there are a lot of people in the park. There is a sign that says autumn camp. There are a lot of people in the park. There are.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 27: 108.0s - 113.0s\n   Generated caption: In the video, there are three people standing in front of a sign that says autumn camp. One of the people is holding a sign that says autumn camp. The second person is holding a sign that says autumn camp. The third person is holding a sign that says autumn camp.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 28: 112.0s - 117.0s\n   Generated caption: 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 29: 116.0s - 121.0s\n   Generated caption: 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 30: 120.0s - 125.0s\n   Generated caption: The video shows a group of people talking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 31: 124.0s - 129.0s\n   Generated caption: The two girls are looking at each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 32: 128.0s - 133.0s\n   Generated caption: The girl is looking at the camera. The girl is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 33: 132.0s - 137.0s\n   Generated caption: 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙�.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 34: 136.0s - 141.0s\n   Generated caption: The woman is adjusting the hat on the man's head. The woman is adjusting the hat on the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 35: 140.0s - 145.0s\n   Generated caption: In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 36: 144.0s - 149.0s\n   Generated caption: The woman is looking at the sky.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 37: 148.0s - 153.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 38: 152.0s - 157.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 39: 156.0s - 161.0s\n   Generated caption: The video shows a man and a woman talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 40: 160.0s - 165.0s\n   Generated caption: In the video, there are a lot of people sitting in a room. There are a lot of people sitting in a room. There are a lot of people sitting in.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 41: 164.0s - 169.0s\n   Generated caption: The person in the video is giving a hug to a stuffed animal.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 42: 168.0s - 173.0s\n   Generated caption: The woman is sitting on the chair. The woman is looking at the stuffed animal. The woman is holding the stuffed animal. The woman is holding the stuffed.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 43: 172.0s - 177.0s\n   Generated caption: The woman is sitting on the chair. The woman is holding a green apple. The woman is smiling. The woman is looking at the camera. The woman is looking at the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 44: 176.0s - 181.0s\n   Generated caption: The girl is eating the doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 45: 180.0s - 185.0s\n   Generated caption: The woman is sitting on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 46: 184.0s - 189.0s\n   Generated caption: A woman is holding a piece of paper with a question mark on it. A.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 47: 188.0s - 193.0s\n   Generated caption: A woman is holding a doughnut in her hands. A woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 48: 192.0s - 197.0s\n   Generated caption: A woman is sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 49: 196.0s - 201.0s\n   Generated caption: A woman is holding a doughnut in her hands.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 50: 200.0s - 205.0s\n   Generated caption: The woman is holding a piece of bread in her hands.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 51: 204.0s - 209.0s\n   Generated caption: The woman is looking at a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 52: 208.0s - 213.0s\n   Generated caption: The woman is folding the paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 53: 212.0s - 217.0s\n   Generated caption: The woman is looking at the paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 54: 216.0s - 221.0s\n   Generated caption: A woman is holding a piece of paper in her hand. She is looking at the paper with a surprised look on her face. A woman is holding a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 55: 220.0s - 225.0s\n   Generated caption: A woman is looking at a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 56: 224.0s - 229.0s\n   Generated caption: A woman is sitting at a table. The woman is looking at a piece of paper. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 57: 228.0s - 233.0s\n   Generated caption: A woman is looking at a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 58: 232.0s - 237.0s\n   Generated caption: A woman is sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 59: 236.0s - 241.0s\n   Generated caption: The girl is reading a book.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 60: 240.0s - 245.0s\n   Generated caption: A group of people are sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 61: 244.0s - 249.0s\n   Generated caption: The woman is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 62: 248.0s - 253.0s\n   Generated caption: In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a jacket. The man is holding a piece of paper and the woman is holding a piece of paper. The man is looking at the woman and the woman is looking at the man. The man is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 63: 252.0s - 257.0s\n   Generated caption: A group of people are standing in a park. One of the people is holding a camera. The other people are looking at the camera. One of the people is looking at the camera and the other people are looking at the camera. This video is part of the following collections  Description: A group of people are standing in a park. Description: A group of people are standing in a.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 64: 256.0s - 261.0s\n   Generated caption: A man and a woman are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 65: 260.0s - 265.0s\n   Generated caption: 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 66: 264.0s - 269.0s\n   Generated caption: In the video, there is a man and a woman sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 67: 268.0s - 273.0s\n   Generated caption: The video shows a man and a woman talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 68: 272.0s - 277.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 69: 276.0s - 281.0s\n   Generated caption: A man and a woman are walking in the park. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 70: 280.0s - 285.0s\n   Generated caption: A man and a woman are walking in a park. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 71: 284.0s - 289.0s\n   Generated caption: 고맙습니다  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 72: 288.0s - 293.0s\n   Generated caption: The video shows a group of people. Question.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 73: 292.0s - 297.0s\n   Generated caption: In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 74: 296.0s - 301.0s\n   Generated caption: In the video, there is a man and a woman. The man is holding a camera. The woman is looking at the camera. The man is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 75: 300.0s - 305.0s\n   Generated caption: The two girls are talking to each other. The girl on the left is looking at the girl on the right. The girl on the right is looking at the girl on the left. The girl on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 76: 304.0s - 309.0s\n   Generated caption: The two girls are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 77: 308.0s - 313.0s\n   Generated caption: 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 78: 312.0s - 317.0s\n   Generated caption: A group of people are standing in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 79: 316.0s - 321.0s\n   Generated caption: A man and a woman are talking to each other. A man and.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 80: 320.0s - 325.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 81: 324.0s - 329.0s\n   Generated caption: A man and a woman are walking together.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 82: 328.0s - 333.0s\n   Generated caption: In the video, there is a man and a woman talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 83: 332.0s - 337.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 84: 336.0s - 341.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 85: 340.0s - 345.0s\n   Generated caption: A group of people are sitting on the grass. One of the people is holding a bag. The other people are looking at the bag.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 86: 344.0s - 349.0s\n   Generated caption: The video shows a group of people.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 87: 348.0s - 353.0s\n   Generated caption: A group of people are walking in a park. Answer.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 88: 352.0s - 357.0s\n   Generated caption: A man and a woman are playing with a dog.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 89: 356.0s - 361.0s\n   Generated caption: In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 90: 360.0s - 365.0s\n   Generated caption: The video shows a group of people walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 91: 364.0s - 369.0s\n   Generated caption: A group of people are standing in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 92: 368.0s - 373.0s\n   Generated caption: The video shows a group of people talking to each other. Question.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 93: 372.0s - 377.0s\n   Generated caption: The video shows a group of people. The people in the video are smiling. The people in the video are looking at each other. The people in the video are laughing. The people in the video are.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 94: 376.0s - 381.0s\n   Generated caption: The scene shows a group of people. The scene shows.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 95: 380.0s - 385.0s\n   Generated caption: A group of people are standing in a grass field. They are looking at a dog. The dog is lying on the ground. The dog is looking at the people. The people are looking at the dog. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 96: 384.0s - 389.0s\n   Generated caption: The video shows a group of people talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 97: 388.0s - 393.0s\n   Generated caption: A man is talking to a woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 98: 392.0s - 397.0s\n   Generated caption: A group of people are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 99: 396.0s - 401.0s\n   Generated caption: A group of people are playing a game of frisbee in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 100: 400.0s - 405.0s\n   Generated caption: The video shows a group of people sitting around a table. The table is covered with a white table cloth. There are several chairs arranged around the table. One of the chairs has a woman sitting on it. The woman is holding a cup of coffee in her hand. A man is sitting on the chair next to the woman. The man is holding a cup of coffee in his hand. The man is wearing a white shirt and blue jeans. The man is also holding a cup of coffee in his hand. A woman is sitting on the chair next to the man. The woman is holding a cup.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 101: 404.0s - 409.0s\n   Generated caption: The video starts with a shot of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his right hand. He is looking at the camera with a smile on his face. The next shot is of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 102: 408.0s - 413.0s\n   Generated caption: A man and a woman are having a picnic in the park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 103: 412.0s - 417.0s\n   Generated caption: A group of people are playing a game of badminton.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 104: 416.0s - 421.0s\n   Generated caption: The two women are playing a game of tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 105: 420.0s - 425.0s\n   Generated caption: A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 106: 424.0s - 429.0s\n   Generated caption: A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 107: 428.0s - 433.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 108: 432.0s - 437.0s\n   Generated caption: The woman is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 109: 436.0s - 441.0s\n   Generated caption: The woman is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 110: 440.0s - 445.0s\n   Generated caption: A group of people are playing a game of tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 111: 444.0s - 449.0s\n   Generated caption: A man and a woman are playing a game of kite flying. A man and a woman are playing a.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 112: 448.0s - 453.0s\n   Generated caption: A man and a woman are playing a game of kite flying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 113: 452.0s - 457.0s\n   Generated caption: A man and a woman are playing a game of tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 114: 456.0s - 461.0s\n   Generated caption: A man is playing tennis with a woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 115: 460.0s - 465.0s\n   Generated caption: A man and a woman are having a picnic. The man is sitting on a chair. The woman is sitting on a chair. The man and the woman are having a picnic. The man and the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 116: 464.0s - 469.0s\n   Generated caption: A man and a woman are playing a game of tennis. The man is holding a tennis racket. The woman is holding a tennis ball. The man is hitting the tennis ball with the tennis racket. The woman is hitting the tennis ball with the tennis ball.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 117: 468.0s - 473.0s\n   Generated caption: A man and a woman are having a picnic in a park. The man is sitting on a chair and the woman is sitting on the ground. The man is holding a cup of coffee and the woman is holding a cup of tea. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 118: 472.0s - 477.0s\n   Generated caption: The woman is playing with the man's hat.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 119: 476.0s - 481.0s\n   Generated caption: The two women are playing a game of \"magnets\" on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 120: 480.0s - 485.0s\n   Generated caption: The man and the woman are playing with the toy. The man is holding the toy in his hand. The woman is looking at the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 121: 484.0s - 489.0s\n   Generated caption: The two women are playing a game of dominoes. The woman on the left is playing dominoes with the woman on the right. The woman on the right is playing dominoes with the woman on the left.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 122: 488.0s - 493.0s\n   Generated caption: The two women are playing a game of dominoes.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 123: 492.0s - 497.0s\n   Generated caption: A man and a woman are playing a game of dominoes. The man is holding the dominoes in his hands. The woman is sitting on a chair. The man is holding the dom.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 124: 496.0s - 501.0s\n   Generated caption: The two women are playing with the toy. One of them is looking at the toy and the other one is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 125: 500.0s - 505.0s\n   Generated caption: The girl is playing a game with the boy. The boy is playing a game with the girl.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 126: 504.0s - 509.0s\n   Generated caption: The two girls are playing a game. The girl on the right is playing a game with the girl on the left. The girl on the left is playing a game with the girl on the right. The girl on the right is playing a game.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 127: 508.0s - 513.0s\n   Generated caption: The two women are playing a game of dominoes.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 128: 512.0s - 517.0s\n   Generated caption: The girl is playing with the toy.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 129: 516.0s - 521.0s\n   Generated caption: The two women are playing a game of tic-tac-toe.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 130: 520.0s - 525.0s\n   Generated caption: The two women are sitting on the table. One of them is looking at the other one. The other one is looking at the other one. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 131: 524.0s - 529.0s\n   Generated caption: A man and a woman are sitting on a bean bag chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 132: 528.0s - 533.0s\n   Generated caption: A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 133: 532.0s - 537.0s\n   Generated caption: The man is looking out the window. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 134: 536.0s - 541.0s\n   Generated caption: The man is looking out of the window.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 135: 540.0s - 545.0s\n   Generated caption: A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 136: 544.0s - 549.0s\n   Generated caption: The woman is sitting at the table. The woman is looking at the camera. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 137: 548.0s - 553.0s\n   Generated caption: A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 138: 552.0s - 557.0s\n   Generated caption: A man and a woman are sitting at a table. The man looks at the woman. The woman looks at the man. The woman looks at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 139: 556.0s - 561.0s\n   Generated caption: A man and a woman are looking at each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 140: 560.0s - 565.0s\n   Generated caption: The girl is sleeping in the chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 141: 564.0s - 569.0s\n   Generated caption: The woman is sleeping in the chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 142: 568.0s - 573.0s\n   Generated caption: A man and a woman are sitting on the ground. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 143: 572.0s - 577.0s\n   Generated caption: The woman is playing a game of badminton with her friend.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 144: 576.0s - 581.0s\n   Generated caption: A man and a woman are playing a game of frisbee in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 145: 580.0s - 585.0s\n   Generated caption: A group of people are playing a game of frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 146: 584.0s - 589.0s\n   Generated caption: The woman is throwing the frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 147: 588.0s - 593.0s\n   Generated caption: A man throws a frisbee to a woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 148: 592.0s - 597.0s\n   Generated caption: A man and a woman are swinging on a swing.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 149: 596.0s - 601.0s\n   Generated caption: A man and a woman are swinging on a swing.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 150: 600.0s - 605.0s\n   Generated caption: A man and a woman are sleeping on the ground. The man is holding the woman's hand. The woman is holding the man's hand.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 151: 604.0s - 609.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 152: 608.0s - 613.0s\n   Generated caption: The woman is sitting on the chair. The woman is holding a cup of tea. The woman is drinking tea.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 153: 612.0s - 617.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The woman is looking at the man. The man is looking at the woman. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 154: 616.0s - 621.0s\n   Generated caption: The girl is sitting on the chair. The girl is looking at something. The girl is drinking something. The girl is eating something. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 155: 620.0s - 625.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 156: 624.0s - 629.0s\n   Generated caption: The woman is sitting on a chair. The man is sitting on a chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 157: 628.0s - 633.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 158: 632.0s - 637.0s\n   Generated caption: The man is sitting on the chair. The woman is sitting on the chair. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 159: 636.0s - 641.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 160: 640.0s - 645.0s\n   Generated caption: A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 161: 644.0s - 649.0s\n   Generated caption: The man is sitting on the chair. The woman is sitting on the chair. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 162: 648.0s - 653.0s\n   Generated caption: A man and a woman are sitting at a picnic table. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 163: 652.0s - 657.0s\n   Generated caption: The girl is sitting on the chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 164: 656.0s - 661.0s\n   Generated caption: The girl is eating a cake. 정말 기존  정말 기존  정말 기존  정말 기존  정말 기존  정말 기존  정말 기존  정말.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 165: 660.0s - 665.0s\n   Generated caption: The girl is eating a piece of cake. 숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙�.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 166: 664.0s - 669.0s\n   Generated caption: A man and a woman are sitting at a table. The man is looking at a book. The woman is looking at the man. The man is looking at the woman.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 167: 668.0s - 673.0s\n   Generated caption: The woman is looking at the book. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 168: 672.0s - 677.0s\n   Generated caption: The two women are sitting on the chairs. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 169: 676.0s - 681.0s\n   Generated caption: In the video, there is a man and a woman sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 170: 680.0s - 685.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 171: 684.0s - 689.0s\n   Generated caption: The video shows a group of people sitting around a table. One of the people is holding a piece of paper. Another person is looking at the paper. The third person is looking at the paper. The fourth person is looking at the paper. The fifth person is looking at the paper. The sixth person is looking at the paper. The seventh person is looking at the paper. The eighth person is looking at the paper. The ninth person is looking at the paper. The tenth person is looking at the paper. The eleventh person is looking.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 172: 688.0s - 693.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 173: 692.0s - 697.0s\n   Generated caption: A group of people are sitting around a table. One of the people is holding a piece of paper. The other people are looking at the piece of paper. The person holding the piece of paper is looking at the people looking at the piece of paper. The person holding the piece of paper is looking at the people.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 174: 696.0s - 701.0s\n   Generated caption: The girl is sitting on the chair. The girl is holding a carrot. The girl is looking at the camera. The girl is smiling.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 175: 700.0s - 705.0s\n   Generated caption: The woman is looking at the paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 176: 704.0s - 709.0s\n   Generated caption: In the video, there are two women sitting in a tent. One of them is wearing a baseball cap and the other one is not wearing a baseball cap. The woman wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is not wearing the baseball cap is looking at the woman who is wearing the baseball cap. The woman who is wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is wearing the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 177: 708.0s - 713.0s\n   Generated caption: In the video, there is a man and a woman sitting on a bench. The man is wearing a baseball cap. The woman is wearing a baseball cap. The woman is wearing.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 178: 712.0s - 717.0s\n   Generated caption: A group of people are sitting in a tent. Question.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 179: 716.0s - 721.0s\n   Generated caption: The video shows a group of people sitting around a table. The video then cuts to a close-up of one of the people at the table. The person is holding a cup of tea. The video then cuts to a close-up of another person at the table. The person is also holding a cup of tea. The video then cuts to a close-up of a third person at the table. The video then cuts to a close-up of a fourth person at the table. The person is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 180: 720.0s - 725.0s\n   Generated caption: A man and a woman are standing in front of a sign that says \"Manto Market\".\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 181: 724.0s - 729.0s\n   Generated caption: The two women are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 182: 728.0s - 733.0s\n   Generated caption: A man and a woman are walking in a tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 183: 732.0s - 737.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 184: 736.0s - 741.0s\n   Generated caption: The woman is looking at something in the tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 185: 740.0s - 745.0s\n   Generated caption: The woman is looking at something in the tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 186: 744.0s - 749.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 187: 748.0s - 753.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 188: 752.0s - 757.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 189: 756.0s - 761.0s\n   Generated caption: A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 190: 760.0s - 765.0s\n   Generated caption: The two women are playing a game of badminton.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 191: 764.0s - 769.0s\n   Generated caption: A group of people are playing a game of frisbee in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 192: 768.0s - 773.0s\n   Generated caption: The woman is throwing the frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 193: 772.0s - 777.0s\n   Generated caption: A woman throws a frisbee in the air.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 194: 776.0s - 781.0s\n   Generated caption: The woman is sitting on the chair. The woman is holding a tennis ball. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 195: 780.0s - 785.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The man is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 196: 784.0s - 789.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at something on the table. The woman is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 197: 788.0s - 793.0s\n   Generated caption: The woman takes the camera from the man and looks at it.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 198: 792.0s - 797.0s\n   Generated caption: The two women look at each other. The woman on the right looks at the woman on the left. The woman on the left looks at the woman on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 199: 796.0s - 801.0s\n   Generated caption: The woman knits a sweater.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 200: 800.0s - 805.0s\n   Generated caption: The woman knits a sweater.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 201: 804.0s - 809.0s\n   Generated caption: The woman is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 202: 808.0s - 813.0s\n   Generated caption: The woman is sitting on the chair. The woman is looking at the table. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 203: 812.0s - 817.0s\n   Generated caption: The two women are looking at each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 204: 816.0s - 821.0s\n   Generated caption: The woman is looking at something. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 205: 820.0s - 825.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 206: 824.0s - 829.0s\n   Generated caption: A man and a woman are standing in front of a tent. The man is holding a camera. The woman is looking at the camera. The man is talking to the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 207: 828.0s - 833.0s\n   Generated caption: A man and a woman are sitting under an umbrella. The man is holding a gun and the woman is holding a knife. The man is pointing the gun at the woman. The woman is pointing the knife at the man. The woman is pointing the.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 208: 832.0s - 837.0s\n   Generated caption: A group of people are standing under an umbrella. The umbrella is being held by a man and a woman. The umbrella is being held.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 209: 836.0s - 841.0s\n   Generated caption: The two girls are talking to each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 210: 840.0s - 845.0s\n   Generated caption: The two girls are hugging each other.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 211: 844.0s - 849.0s\n   Generated caption: The two women are talking to each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 212: 848.0s - 853.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 213: 852.0s - 857.0s\n   Generated caption: The woman looks at the woman's hand.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 214: 856.0s - 861.0s\n   Generated caption: A woman is sitting in a tent. Woman: A woman is sitting in a tent. Camer.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 215: 860.0s - 865.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 216: 864.0s - 869.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 217: 868.0s - 873.0s\n   Generated caption: A group of people are sitting in a tent.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 218: 872.0s - 877.0s\n   Generated caption: The woman is sitting on the chair. The man is sitting on the chair. The woman is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 219: 876.0s - 881.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 220: 880.0s - 885.0s\n   Generated caption: A man and a woman are playing a game of frisbee in a park. A man and a woman are.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 221: 884.0s - 889.0s\n   Generated caption: A man and a woman are sitting under an umbrella.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 222: 888.0s - 893.0s\n   Generated caption: A man and a woman are playing a game of frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 223: 892.0s - 897.0s\n   Generated caption: A man and a woman are sitting in a lawn chair. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 224: 896.0s - 901.0s\n   Generated caption: The woman is sitting in a lawn chair. The woman is holding a tennis racket. The woman is holding a tennis ball. The woman is holding a tennis.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 225: 900.0s - 905.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 226: 904.0s - 909.0s\n   Generated caption: The woman is sitting on the chair. The woman is looking at something.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 227: 908.0s - 913.0s\n   Generated caption: The woman is kneeling down on the floor.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 228: 912.0s - 917.0s\n   Generated caption: A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 229: 916.0s - 921.0s\n   Generated caption: The two women are talking to each other. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 230: 920.0s - 925.0s\n   Generated caption: A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 231: 924.0s - 929.0s\n   Generated caption: A man and a woman are playing a game of frisbee. The man throws the frisbee to the woman. The woman catches the frisbee and throws it back to the man. The man catches the frisbee and throws it back to the woman. The woman catches the fr.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 232: 928.0s - 933.0s\n   Generated caption: The video shows a girl playing with a frisbee.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 233: 932.0s - 937.0s\n   Generated caption: In the video, a man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 234: 936.0s - 941.0s\n   Generated caption: A group of people are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 235: 940.0s - 945.0s\n   Generated caption: A group of people are sitting around a table. A woman is talking to a man. A man is talking to a woman. A.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 236: 944.0s - 949.0s\n   Generated caption: A woman is holding a fly with a net.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 237: 948.0s - 953.0s\n   Generated caption: A woman is sitting on a chair.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 238: 952.0s - 957.0s\n   Generated caption: A woman is eating an apple.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 239: 956.0s - 961.0s\n   Generated caption: A woman is sitting at a table. The woman is holding an apple. The woman is looking at the apple. The woman is holding the apple. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 240: 960.0s - 965.0s\n   Generated caption: The girl is eating a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 241: 964.0s - 969.0s\n   Generated caption: A woman is sitting at a table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 242: 968.0s - 973.0s\n   Generated caption: A woman is holding a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 243: 972.0s - 977.0s\n   Generated caption: In the video, there is a girl who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 244: 976.0s - 981.0s\n   Generated caption: The girl is eating a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 245: 980.0s - 985.0s\n   Generated caption: The girl is playing with a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 246: 984.0s - 989.0s\n   Generated caption: A woman is eating a doughnut.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 247: 988.0s - 993.0s\n   Generated caption: The woman is holding a piece of paper.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 248: 992.0s - 997.0s\n   Generated caption: The woman is sitting at the table. The woman is holding a piece of paper. The.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 249: 996.0s - 1001.0s\n   Generated caption: The girl is sitting on the table. The girl is eating an apple.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 250: 1000.0s - 1005.0s\n   Generated caption: In the video, there is a woman who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 251: 1004.0s - 1009.0s\n   Generated caption: In the video, there is a girl who is sitting on a chair. She is looking at something on the table.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 252: 1008.0s - 1013.0s\n   Generated caption: There are two women sitting at a table. One of the women is talking to the other woman. The other woman is listening to what the first woman is saying.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 253: 1012.0s - 1017.0s\n   Generated caption: The girl is playing with the dog. The girl is.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 254: 1016.0s - 1021.0s\n   Generated caption: The girl is tying the rope to the tree.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 255: 1020.0s - 1025.0s\n   Generated caption: A man and a woman are walking in a park.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 256: 1024.0s - 1029.0s\n   Generated caption: A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 257: 1028.0s - 1033.0s\n   Generated caption: The man is trying to cover his face with his hands. The man is trying to cover his face.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 258: 1032.0s - 1037.0s\n   Generated caption: The woman is walking on the bridge. Catch the latest news, live coverage and in-depth analyses from India and World. Follow us on Facebook and Twitter.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 259: 1036.0s - 1041.0s\n   Generated caption: Describe in detail what actions are taking place in the video?.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 260: 1040.0s - 1045.0s\n   Generated caption: Describe in detail what actions are taking place in the video. The man is looking at the woman. The woman is looking at the man.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 261: 1044.0s - 1049.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 262: 1048.0s - 1053.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 263: 1052.0s - 1057.0s\n   Generated caption: Describe in detail what actions are taking place in the video?.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 264: 1056.0s - 1061.0s\n   Generated caption: The girl is looking at the camera.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 265: 1060.0s - 1065.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 266: 1064.0s - 1069.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 267: 1068.0s - 1073.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 268: 1072.0s - 1077.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 269: 1076.0s - 1081.0s\n   Generated caption: Describe in detail what actions are taking place in the video. The girl is flying a kite.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 270: 1080.0s - 1085.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 271: 1084.0s - 1089.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Segment 272: 1088.0s - 1093.0s\n   Generated caption: Describe in detail what actions are taking place in the video.\nReached end of video during sliding window update.\nTotal segmentation and caption generation time: 5877.1s\nSegment 0 (0.0s - 5.0s): A man and a woman are walking on a bridge. The man is wearing a baseball cap and the woman is wearing a hat. The man is talking to the woman. The woman is looking at the man. The man is looking at the woman. The woman is looking at the.\nSegment 1 (4.0s - 9.0s): A group of people are playing a game of kite flying.\nSegment 2 (8.0s - 13.0s): A group of people are playing a game of kick the can.\nSegment 3 (12.0s - 17.0s): A group of people are riding a scooter. Categories.\nSegment 4 (16.0s - 21.0s): In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a hat. The man is holding a fishing rod and the woman is holding a fishing pole. The man is looking at the woman and the woman is looking at the man. The woman is smiling and the man is smiling. The man is looking at the woman and the woman is looking at.\nSegment 5 (20.0s - 25.0s): A group of people are sitting around a table. They are eating some food. One of the people is holding a chopstick. The other person is holding a bowl of food. The third person is holding a bowl of food. The fourth person is holding a bowl of food. The fifth person is holding a bowl of food. The sixth person is holding a bowl of food. The seventh person is holding a bowl of food. The eighth person is holding a bowl of food. The ninth person is holding a bowl of food. The tenth person.\nSegment 6 (24.0s - 29.0s): A group of people are sitting around a fire pit. One of the people is using a cell phone. The other people are talking to each other. The other people.\nSegment 7 (28.0s - 33.0s): A man and a woman are sitting on a bench. The man is looking at his phone. The woman is looking at her phone.\nSegment 8 (32.0s - 37.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 9 (36.0s - 41.0s): A man and a woman are sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\nSegment 10 (40.0s - 45.0s): In the video, there is a man and a woman sitting in chairs. The man is talking to the woman. The woman is looking at the man. The woman is looking at.\nSegment 11 (44.0s - 49.0s): In the video, there are three people sitting in a lawn chair.\nSegment 12 (48.0s - 53.0s): The image shows a stream of water flowing through a forest.\nSegment 13 (52.0s - 57.0s): The image shows a stream of water flowing through a grassy area. Other: The image shows a stream of water flowing through a grassy area.\nSegment 14 (56.0s - 61.0s): The picture shows a group of people crossing a bridge.\nSegment 15 (60.0s - 65.0s): A man and a woman are walking on a bridge.\nSegment 16 (64.0s - 69.0s): A man and a woman are walking on a bridge.\nSegment 17 (68.0s - 73.0s): A group of people are walking on a bridge.\nSegment 18 (72.0s - 77.0s): A man and a woman are walking on a bridge. The man is looking at the woman. The woman is looking at the man.\nSegment 19 (76.0s - 81.0s): A man and a woman are walking on a bridge.\nSegment 20 (80.0s - 85.0s): A group of people are walking on a bridge.\nSegment 21 (84.0s - 89.0s): A group of people are walking on a bridge.\nSegment 22 (88.0s - 93.0s): A group of people are walking in a park.\nSegment 23 (92.0s - 97.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 24 (96.0s - 101.0s): A man is talking to a woman. Woman: A man is talking to a woman. Camer.\nSegment 25 (100.0s - 105.0s): A group of people are walking in a park.\nSegment 26 (104.0s - 109.0s): In the video, there are a lot of people in the park. There is a sign that says autumn camp. There are a lot of people in the park. There are.\nSegment 27 (108.0s - 113.0s): In the video, there are three people standing in front of a sign that says autumn camp. One of the people is holding a sign that says autumn camp. The second person is holding a sign that says autumn camp. The third person is holding a sign that says autumn camp.\nSegment 28 (112.0s - 117.0s): 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\nSegment 29 (116.0s - 121.0s): 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\nSegment 30 (120.0s - 125.0s): The video shows a group of people talking.\nSegment 31 (124.0s - 129.0s): The two girls are looking at each other.\nSegment 32 (128.0s - 133.0s): The girl is looking at the camera. The girl is.\nSegment 33 (132.0s - 137.0s): 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙�.\nSegment 34 (136.0s - 141.0s): The woman is adjusting the hat on the man's head. The woman is adjusting the hat on the.\nSegment 35 (140.0s - 145.0s): In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\nSegment 36 (144.0s - 149.0s): The woman is looking at the sky.\nSegment 37 (148.0s - 153.0s): In the video, there is a man and a woman who are talking to each other.\nSegment 38 (152.0s - 157.0s): In the video, there is a man and a woman who are talking to each other.\nSegment 39 (156.0s - 161.0s): The video shows a man and a woman talking to each other.\nSegment 40 (160.0s - 165.0s): In the video, there are a lot of people sitting in a room. There are a lot of people sitting in a room. There are a lot of people sitting in.\nSegment 41 (164.0s - 169.0s): The person in the video is giving a hug to a stuffed animal.\nSegment 42 (168.0s - 173.0s): The woman is sitting on the chair. The woman is looking at the stuffed animal. The woman is holding the stuffed animal. The woman is holding the stuffed.\nSegment 43 (172.0s - 177.0s): The woman is sitting on the chair. The woman is holding a green apple. The woman is smiling. The woman is looking at the camera. The woman is looking at the.\nSegment 44 (176.0s - 181.0s): The girl is eating the doughnut.\nSegment 45 (180.0s - 185.0s): The woman is sitting on the table.\nSegment 46 (184.0s - 189.0s): A woman is holding a piece of paper with a question mark on it. A.\nSegment 47 (188.0s - 193.0s): A woman is holding a doughnut in her hands. A woman is.\nSegment 48 (192.0s - 197.0s): A woman is sitting at a table.\nSegment 49 (196.0s - 201.0s): A woman is holding a doughnut in her hands.\nSegment 50 (200.0s - 205.0s): The woman is holding a piece of bread in her hands.\nSegment 51 (204.0s - 209.0s): The woman is looking at a piece of paper.\nSegment 52 (208.0s - 213.0s): The woman is folding the paper.\nSegment 53 (212.0s - 217.0s): The woman is looking at the paper.\nSegment 54 (216.0s - 221.0s): A woman is holding a piece of paper in her hand. She is looking at the paper with a surprised look on her face. A woman is holding a piece of paper.\nSegment 55 (220.0s - 225.0s): A woman is looking at a piece of paper.\nSegment 56 (224.0s - 229.0s): A woman is sitting at a table. The woman is looking at a piece of paper. The.\nSegment 57 (228.0s - 233.0s): A woman is looking at a piece of paper.\nSegment 58 (232.0s - 237.0s): A woman is sitting at a table.\nSegment 59 (236.0s - 241.0s): The girl is reading a book.\nSegment 60 (240.0s - 245.0s): A group of people are sitting at a table.\nSegment 61 (244.0s - 249.0s): The woman is looking at the camera.\nSegment 62 (248.0s - 253.0s): In the video, there is a man and a woman who are talking to each other. The man is wearing a baseball cap and the woman is wearing a jacket. The man is holding a piece of paper and the woman is holding a piece of paper. The man is looking at the woman and the woman is looking at the man. The man is looking at.\nSegment 63 (252.0s - 257.0s): A group of people are standing in a park. One of the people is holding a camera. The other people are looking at the camera. One of the people is looking at the camera and the other people are looking at the camera. This video is part of the following collections  Description: A group of people are standing in a park. Description: A group of people are standing in a.\nSegment 64 (256.0s - 261.0s): A man and a woman are talking to each other.\nSegment 65 (260.0s - 265.0s): 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\nSegment 66 (264.0s - 269.0s): In the video, there is a man and a woman sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking.\nSegment 67 (268.0s - 273.0s): The video shows a man and a woman talking to each other.\nSegment 68 (272.0s - 277.0s): The girl is looking at the camera.\nSegment 69 (276.0s - 281.0s): A man and a woman are walking in the park. The man is looking at the woman. The woman is looking at the man.\nSegment 70 (280.0s - 285.0s): A man and a woman are walking in a park. The man is looking at the woman. The woman is looking at the man.\nSegment 71 (284.0s - 289.0s): 고맙습니다  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글  댓글.\nSegment 72 (288.0s - 293.0s): The video shows a group of people. Question.\nSegment 73 (292.0s - 297.0s): In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\nSegment 74 (296.0s - 301.0s): In the video, there is a man and a woman. The man is holding a camera. The woman is looking at the camera. The man is looking at the camera.\nSegment 75 (300.0s - 305.0s): The two girls are talking to each other. The girl on the left is looking at the girl on the right. The girl on the right is looking at the girl on the left. The girl on the right.\nSegment 76 (304.0s - 309.0s): The two girls are talking to each other.\nSegment 77 (308.0s - 313.0s): 고맙습니다  고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙습니다 고맙.\nSegment 78 (312.0s - 317.0s): A group of people are standing in a park.\nSegment 79 (316.0s - 321.0s): A man and a woman are talking to each other. A man and.\nSegment 80 (320.0s - 325.0s): A group of people are walking in a park.\nSegment 81 (324.0s - 329.0s): A man and a woman are walking together.\nSegment 82 (328.0s - 333.0s): In the video, there is a man and a woman talking to each other.\nSegment 83 (332.0s - 337.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 84 (336.0s - 341.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 85 (340.0s - 345.0s): A group of people are sitting on the grass. One of the people is holding a bag. The other people are looking at the bag.\nSegment 86 (344.0s - 349.0s): The video shows a group of people.\nSegment 87 (348.0s - 353.0s): A group of people are walking in a park. Answer.\nSegment 88 (352.0s - 357.0s): A man and a woman are playing with a dog.\nSegment 89 (356.0s - 361.0s): In the video, there is a man and a woman. The man is looking at the woman. The woman is looking at the man.\nSegment 90 (360.0s - 365.0s): The video shows a group of people walking in a park.\nSegment 91 (364.0s - 369.0s): A group of people are standing in a park.\nSegment 92 (368.0s - 373.0s): The video shows a group of people talking to each other. Question.\nSegment 93 (372.0s - 377.0s): The video shows a group of people. The people in the video are smiling. The people in the video are looking at each other. The people in the video are laughing. The people in the video are.\nSegment 94 (376.0s - 381.0s): The scene shows a group of people. The scene shows.\nSegment 95 (380.0s - 385.0s): A group of people are standing in a grass field. They are looking at a dog. The dog is lying on the ground. The dog is looking at the people. The people are looking at the dog. The.\nSegment 96 (384.0s - 389.0s): The video shows a group of people talking to each other.\nSegment 97 (388.0s - 393.0s): A man is talking to a woman.\nSegment 98 (392.0s - 397.0s): A group of people are talking to each other.\nSegment 99 (396.0s - 401.0s): A group of people are playing a game of frisbee in a park.\nSegment 100 (400.0s - 405.0s): The video shows a group of people sitting around a table. The table is covered with a white table cloth. There are several chairs arranged around the table. One of the chairs has a woman sitting on it. The woman is holding a cup of coffee in her hand. A man is sitting on the chair next to the woman. The man is holding a cup of coffee in his hand. The man is wearing a white shirt and blue jeans. The man is also holding a cup of coffee in his hand. A woman is sitting on the chair next to the man. The woman is holding a cup.\nSegment 101 (404.0s - 409.0s): The video starts with a shot of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his right hand. He is looking at the camera with a smile on his face. The next shot is of a man sitting on a chair. He is holding a cup of coffee in his left hand and a straw in his.\nSegment 102 (408.0s - 413.0s): A man and a woman are having a picnic in the park.\nSegment 103 (412.0s - 417.0s): A group of people are playing a game of badminton.\nSegment 104 (416.0s - 421.0s): The two women are playing a game of tennis.\nSegment 105 (420.0s - 425.0s): A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\nSegment 106 (424.0s - 429.0s): A man and a woman are sitting on the grass. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\nSegment 107 (428.0s - 433.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 108 (432.0s - 437.0s): The woman is playing with the toy.\nSegment 109 (436.0s - 441.0s): The woman is playing with the toy.\nSegment 110 (440.0s - 445.0s): A group of people are playing a game of tennis.\nSegment 111 (444.0s - 449.0s): A man and a woman are playing a game of kite flying. A man and a woman are playing a.\nSegment 112 (448.0s - 453.0s): A man and a woman are playing a game of kite flying.\nSegment 113 (452.0s - 457.0s): A man and a woman are playing a game of tennis.\nSegment 114 (456.0s - 461.0s): A man is playing tennis with a woman.\nSegment 115 (460.0s - 465.0s): A man and a woman are having a picnic. The man is sitting on a chair. The woman is sitting on a chair. The man and the woman are having a picnic. The man and the.\nSegment 116 (464.0s - 469.0s): A man and a woman are playing a game of tennis. The man is holding a tennis racket. The woman is holding a tennis ball. The man is hitting the tennis ball with the tennis racket. The woman is hitting the tennis ball with the tennis ball.\nSegment 117 (468.0s - 473.0s): A man and a woman are having a picnic in a park. The man is sitting on a chair and the woman is sitting on the ground. The man is holding a cup of coffee and the woman is holding a cup of tea. The man is looking at the woman. The woman is looking at the man.\nSegment 118 (472.0s - 477.0s): The woman is playing with the man's hat.\nSegment 119 (476.0s - 481.0s): The two women are playing a game of \"magnets\" on the table.\nSegment 120 (480.0s - 485.0s): The man and the woman are playing with the toy. The man is holding the toy in his hand. The woman is looking at the toy.\nSegment 121 (484.0s - 489.0s): The two women are playing a game of dominoes. The woman on the left is playing dominoes with the woman on the right. The woman on the right is playing dominoes with the woman on the left.\nSegment 122 (488.0s - 493.0s): The two women are playing a game of dominoes.\nSegment 123 (492.0s - 497.0s): A man and a woman are playing a game of dominoes. The man is holding the dominoes in his hands. The woman is sitting on a chair. The man is holding the dom.\nSegment 124 (496.0s - 501.0s): The two women are playing with the toy. One of them is looking at the toy and the other one is playing with the toy.\nSegment 125 (500.0s - 505.0s): The girl is playing a game with the boy. The boy is playing a game with the girl.\nSegment 126 (504.0s - 509.0s): The two girls are playing a game. The girl on the right is playing a game with the girl on the left. The girl on the left is playing a game with the girl on the right. The girl on the right is playing a game.\nSegment 127 (508.0s - 513.0s): The two women are playing a game of dominoes.\nSegment 128 (512.0s - 517.0s): The girl is playing with the toy.\nSegment 129 (516.0s - 521.0s): The two women are playing a game of tic-tac-toe.\nSegment 130 (520.0s - 525.0s): The two women are sitting on the table. One of them is looking at the other one. The other one is looking at the other one. The.\nSegment 131 (524.0s - 529.0s): A man and a woman are sitting on a bean bag chair. The man is looking at the woman. The woman is looking at the man.\nSegment 132 (528.0s - 533.0s): A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\nSegment 133 (532.0s - 537.0s): The man is looking out the window. The man is.\nSegment 134 (536.0s - 541.0s): The man is looking out of the window.\nSegment 135 (540.0s - 545.0s): A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\nSegment 136 (544.0s - 549.0s): The woman is sitting at the table. The woman is looking at the camera. The woman is.\nSegment 137 (548.0s - 553.0s): A man and a woman are sitting at a table. The man is looking at the woman. The woman is looking at the man.\nSegment 138 (552.0s - 557.0s): A man and a woman are sitting at a table. The man looks at the woman. The woman looks at the man. The woman looks at.\nSegment 139 (556.0s - 561.0s): A man and a woman are looking at each other.\nSegment 140 (560.0s - 565.0s): The girl is sleeping in the chair.\nSegment 141 (564.0s - 569.0s): The woman is sleeping in the chair.\nSegment 142 (568.0s - 573.0s): A man and a woman are sitting on the ground. The man is looking at the woman. The woman is looking at the man.\nSegment 143 (572.0s - 577.0s): The woman is playing a game of badminton with her friend.\nSegment 144 (576.0s - 581.0s): A man and a woman are playing a game of frisbee in a park.\nSegment 145 (580.0s - 585.0s): A group of people are playing a game of frisbee.\nSegment 146 (584.0s - 589.0s): The woman is throwing the frisbee.\nSegment 147 (588.0s - 593.0s): A man throws a frisbee to a woman.\nSegment 148 (592.0s - 597.0s): A man and a woman are swinging on a swing.\nSegment 149 (596.0s - 601.0s): A man and a woman are swinging on a swing.\nSegment 150 (600.0s - 605.0s): A man and a woman are sleeping on the ground. The man is holding the woman's hand. The woman is holding the man's hand.\nSegment 151 (604.0s - 609.0s): The woman is sitting on the chair. The man is sitting on the chair. The man is.\nSegment 152 (608.0s - 613.0s): The woman is sitting on the chair. The woman is holding a cup of tea. The woman is drinking tea.\nSegment 153 (612.0s - 617.0s): The woman is sitting on the chair. The man is sitting on the chair. The woman is looking at the man. The man is looking at the woman. The man is.\nSegment 154 (616.0s - 621.0s): The girl is sitting on the chair. The girl is looking at something. The girl is drinking something. The girl is eating something. The.\nSegment 155 (620.0s - 625.0s): The woman is sitting on the chair. The man is sitting on the chair. The man is.\nSegment 156 (624.0s - 629.0s): The woman is sitting on a chair. The man is sitting on a chair. The man is.\nSegment 157 (628.0s - 633.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 158 (632.0s - 637.0s): The man is sitting on the chair. The woman is sitting on the chair. The woman is.\nSegment 159 (636.0s - 641.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 160 (640.0s - 645.0s): A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\nSegment 161 (644.0s - 649.0s): The man is sitting on the chair. The woman is sitting on the chair. The woman is.\nSegment 162 (648.0s - 653.0s): A man and a woman are sitting at a picnic table. The man is looking at the woman. The woman is looking at the man.\nSegment 163 (652.0s - 657.0s): The girl is sitting on the chair.\nSegment 164 (656.0s - 661.0s): The girl is eating a cake. 정말 기존  정말 기존  정말 기존  정말 기존  정말 기존  정말 기존  정말 기존  정말.\nSegment 165 (660.0s - 665.0s): The girl is eating a piece of cake. 숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙소년단  숙�.\nSegment 166 (664.0s - 669.0s): A man and a woman are sitting at a table. The man is looking at a book. The woman is looking at the man. The man is looking at the woman.\nSegment 167 (668.0s - 673.0s): The woman is looking at the book. The woman is.\nSegment 168 (672.0s - 677.0s): The two women are sitting on the chairs. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\nSegment 169 (676.0s - 681.0s): In the video, there is a man and a woman sitting on a bench. The man is talking to the woman. The woman is looking at the man. The woman is looking.\nSegment 170 (680.0s - 685.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 171 (684.0s - 689.0s): The video shows a group of people sitting around a table. One of the people is holding a piece of paper. Another person is looking at the paper. The third person is looking at the paper. The fourth person is looking at the paper. The fifth person is looking at the paper. The sixth person is looking at the paper. The seventh person is looking at the paper. The eighth person is looking at the paper. The ninth person is looking at the paper. The tenth person is looking at the paper. The eleventh person is looking.\nSegment 172 (688.0s - 693.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 173 (692.0s - 697.0s): A group of people are sitting around a table. One of the people is holding a piece of paper. The other people are looking at the piece of paper. The person holding the piece of paper is looking at the people looking at the piece of paper. The person holding the piece of paper is looking at the people.\nSegment 174 (696.0s - 701.0s): The girl is sitting on the chair. The girl is holding a carrot. The girl is looking at the camera. The girl is smiling.\nSegment 175 (700.0s - 705.0s): The woman is looking at the paper.\nSegment 176 (704.0s - 709.0s): In the video, there are two women sitting in a tent. One of them is wearing a baseball cap and the other one is not wearing a baseball cap. The woman wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is not wearing the baseball cap is looking at the woman who is wearing the baseball cap. The woman who is wearing the baseball cap is looking at the woman who is not wearing the baseball cap. The woman who is wearing the.\nSegment 177 (708.0s - 713.0s): In the video, there is a man and a woman sitting on a bench. The man is wearing a baseball cap. The woman is wearing a baseball cap. The woman is wearing.\nSegment 178 (712.0s - 717.0s): A group of people are sitting in a tent. Question.\nSegment 179 (716.0s - 721.0s): The video shows a group of people sitting around a table. The video then cuts to a close-up of one of the people at the table. The person is holding a cup of tea. The video then cuts to a close-up of another person at the table. The person is also holding a cup of tea. The video then cuts to a close-up of a third person at the table. The video then cuts to a close-up of a fourth person at the table. The person is.\nSegment 180 (720.0s - 725.0s): A man and a woman are standing in front of a sign that says \"Manto Market\".\nSegment 181 (724.0s - 729.0s): The two women are talking to each other.\nSegment 182 (728.0s - 733.0s): A man and a woman are walking in a tent.\nSegment 183 (732.0s - 737.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 184 (736.0s - 741.0s): The woman is looking at something in the tent.\nSegment 185 (740.0s - 745.0s): The woman is looking at something in the tent.\nSegment 186 (744.0s - 749.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\nSegment 187 (748.0s - 753.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 188 (752.0s - 757.0s): The girl is looking at the camera.\nSegment 189 (756.0s - 761.0s): A man and a woman are sitting on a chair. The man is looking at the woman. The woman is looking at the man.\nSegment 190 (760.0s - 765.0s): The two women are playing a game of badminton.\nSegment 191 (764.0s - 769.0s): A group of people are playing a game of frisbee in a park.\nSegment 192 (768.0s - 773.0s): The woman is throwing the frisbee.\nSegment 193 (772.0s - 777.0s): A woman throws a frisbee in the air.\nSegment 194 (776.0s - 781.0s): The woman is sitting on the chair. The woman is holding a tennis ball. The woman is.\nSegment 195 (780.0s - 785.0s): The woman is sitting on the chair. The man is sitting on the chair. The man is.\nSegment 196 (784.0s - 789.0s): A man and a woman are sitting in a tent. The man is looking at something on the table. The woman is looking at something on the table.\nSegment 197 (788.0s - 793.0s): The woman takes the camera from the man and looks at it.\nSegment 198 (792.0s - 797.0s): The two women look at each other. The woman on the right looks at the woman on the left. The woman on the left looks at the woman on the right.\nSegment 199 (796.0s - 801.0s): The woman knits a sweater.\nSegment 200 (800.0s - 805.0s): The woman knits a sweater.\nSegment 201 (804.0s - 809.0s): The woman is looking at something on the table.\nSegment 202 (808.0s - 813.0s): The woman is sitting on the chair. The woman is looking at the table. The woman is.\nSegment 203 (812.0s - 817.0s): The two women are looking at each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\nSegment 204 (816.0s - 821.0s): The woman is looking at something. The woman is.\nSegment 205 (820.0s - 825.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\nSegment 206 (824.0s - 829.0s): A man and a woman are standing in front of a tent. The man is holding a camera. The woman is looking at the camera. The man is talking to the camera.\nSegment 207 (828.0s - 833.0s): A man and a woman are sitting under an umbrella. The man is holding a gun and the woman is holding a knife. The man is pointing the gun at the woman. The woman is pointing the knife at the man. The woman is pointing the.\nSegment 208 (832.0s - 837.0s): A group of people are standing under an umbrella. The umbrella is being held by a man and a woman. The umbrella is being held.\nSegment 209 (836.0s - 841.0s): The two girls are talking to each other.\nSegment 210 (840.0s - 845.0s): The two girls are hugging each other.\nSegment 211 (844.0s - 849.0s): The two women are talking to each other. The woman on the right is looking at the woman on the left. The woman on the left is looking at the woman on the right. The woman on the left.\nSegment 212 (848.0s - 853.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 213 (852.0s - 857.0s): The woman looks at the woman's hand.\nSegment 214 (856.0s - 861.0s): A woman is sitting in a tent. Woman: A woman is sitting in a tent. Camer.\nSegment 215 (860.0s - 865.0s): The girl is looking at the camera.\nSegment 216 (864.0s - 869.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 217 (868.0s - 873.0s): A group of people are sitting in a tent.\nSegment 218 (872.0s - 877.0s): The woman is sitting on the chair. The man is sitting on the chair. The woman is.\nSegment 219 (876.0s - 881.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 220 (880.0s - 885.0s): A man and a woman are playing a game of frisbee in a park. A man and a woman are.\nSegment 221 (884.0s - 889.0s): A man and a woman are sitting under an umbrella.\nSegment 222 (888.0s - 893.0s): A man and a woman are playing a game of frisbee.\nSegment 223 (892.0s - 897.0s): A man and a woman are sitting in a lawn chair. The man is looking at the woman. The woman is looking at the man.\nSegment 224 (896.0s - 901.0s): The woman is sitting in a lawn chair. The woman is holding a tennis racket. The woman is holding a tennis ball. The woman is holding a tennis.\nSegment 225 (900.0s - 905.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 226 (904.0s - 909.0s): The woman is sitting on the chair. The woman is looking at something.\nSegment 227 (908.0s - 913.0s): The woman is kneeling down on the floor.\nSegment 228 (912.0s - 917.0s): A man and a woman are in a tent. The man is looking at the woman. The woman is looking at the man. The.\nSegment 229 (916.0s - 921.0s): The two women are talking to each other. The woman on the left is looking at the woman on the right. The woman on the right is looking at the woman on the left. The woman on the right.\nSegment 230 (920.0s - 925.0s): A man and a woman are sitting in a tent. The man is looking at the woman. The woman is looking at the man.\nSegment 231 (924.0s - 929.0s): A man and a woman are playing a game of frisbee. The man throws the frisbee to the woman. The woman catches the frisbee and throws it back to the man. The man catches the frisbee and throws it back to the woman. The woman catches the fr.\nSegment 232 (928.0s - 933.0s): The video shows a girl playing with a frisbee.\nSegment 233 (932.0s - 937.0s): In the video, a man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man. The woman is looking at.\nSegment 234 (936.0s - 941.0s): A group of people are walking in a park.\nSegment 235 (940.0s - 945.0s): A group of people are sitting around a table. A woman is talking to a man. A man is talking to a woman. A.\nSegment 236 (944.0s - 949.0s): A woman is holding a fly with a net.\nSegment 237 (948.0s - 953.0s): A woman is sitting on a chair.\nSegment 238 (952.0s - 957.0s): A woman is eating an apple.\nSegment 239 (956.0s - 961.0s): A woman is sitting at a table. The woman is holding an apple. The woman is looking at the apple. The woman is holding the apple. The.\nSegment 240 (960.0s - 965.0s): The girl is eating a doughnut.\nSegment 241 (964.0s - 969.0s): A woman is sitting at a table.\nSegment 242 (968.0s - 973.0s): A woman is holding a piece of paper.\nSegment 243 (972.0s - 977.0s): In the video, there is a girl who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\nSegment 244 (976.0s - 981.0s): The girl is eating a doughnut.\nSegment 245 (980.0s - 985.0s): The girl is playing with a doughnut.\nSegment 246 (984.0s - 989.0s): A woman is eating a doughnut.\nSegment 247 (988.0s - 993.0s): The woman is holding a piece of paper.\nSegment 248 (992.0s - 997.0s): The woman is sitting at the table. The woman is holding a piece of paper. The.\nSegment 249 (996.0s - 1001.0s): The girl is sitting on the table. The girl is eating an apple.\nSegment 250 (1000.0s - 1005.0s): In the video, there is a woman who is sitting at a table. She is holding a piece of paper in her hand. She is looking at something on the table.\nSegment 251 (1004.0s - 1009.0s): In the video, there is a girl who is sitting on a chair. She is looking at something on the table.\nSegment 252 (1008.0s - 1013.0s): There are two women sitting at a table. One of the women is talking to the other woman. The other woman is listening to what the first woman is saying.\nSegment 253 (1012.0s - 1017.0s): The girl is playing with the dog. The girl is.\nSegment 254 (1016.0s - 1021.0s): The girl is tying the rope to the tree.\nSegment 255 (1020.0s - 1025.0s): A man and a woman are walking in a park.\nSegment 256 (1024.0s - 1029.0s): A man and a woman are sitting on a bench. The man is looking at the woman. The woman is looking at the man.\nSegment 257 (1028.0s - 1033.0s): The man is trying to cover his face with his hands. The man is trying to cover his face.\nSegment 258 (1032.0s - 1037.0s): The woman is walking on the bridge. Catch the latest news, live coverage and in-depth analyses from India and World. Follow us on Facebook and Twitter.\nSegment 259 (1036.0s - 1041.0s): Describe in detail what actions are taking place in the video?.\nSegment 260 (1040.0s - 1045.0s): Describe in detail what actions are taking place in the video. The man is looking at the woman. The woman is looking at the man.\nSegment 261 (1044.0s - 1049.0s): Describe in detail what actions are taking place in the video.\nSegment 262 (1048.0s - 1053.0s): Describe in detail what actions are taking place in the video.\nSegment 263 (1052.0s - 1057.0s): Describe in detail what actions are taking place in the video?.\nSegment 264 (1056.0s - 1061.0s): The girl is looking at the camera.\nSegment 265 (1060.0s - 1065.0s): Describe in detail what actions are taking place in the video.\nSegment 266 (1064.0s - 1069.0s): Describe in detail what actions are taking place in the video.\nSegment 267 (1068.0s - 1073.0s): Describe in detail what actions are taking place in the video.\nSegment 268 (1072.0s - 1077.0s): Describe in detail what actions are taking place in the video.\nSegment 269 (1076.0s - 1081.0s): Describe in detail what actions are taking place in the video. The girl is flying a kite.\nSegment 270 (1080.0s - 1085.0s): Describe in detail what actions are taking place in the video.\nSegment 271 (1084.0s - 1089.0s): Describe in detail what actions are taking place in the video.\nSegment 272 (1088.0s - 1093.0s): Describe in detail what actions are taking place in the video.\nResult saved to '/kaggle/working/video_captions_opt.json'\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# # ==================================================\n# # 1. 모델 및 프로세서 로드 (FLAN‑T5 기반 EILEV)\n# # ==================================================\n# model_name = \"kpyu/eilev-blip2-flan-t5-xl\"  # 사용하시는 체크포인트에 맞게 수정\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# quant_config = BitsAndBytesConfig(load_in_8bit=True) if device==\"cuda\" else None\n\n# model = VideoBlipForConditionalGeneration.from_pretrained(\n#     model_name,\n#     device_map=\"auto\",\n#     quantization_config=quant_config,\n#     torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n# )\n# processor = AutoProcessor.from_pretrained(model_name)\n# tokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:04:38.465089Z","iopub.execute_input":"2025-03-18T10:04:38.465282Z","iopub.status.idle":"2025-03-18T10:07:07.677986Z","shell.execute_reply.started":"2025-03-18T10:04:38.465265Z","shell.execute_reply":"2025-03-18T10:07:07.677315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"511b8584dfd2498fa80ff9f1ba2a3417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe7005ffb154a8988642b51dd7128f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0005355c574636ab63f9e18828e310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2e2a36fdb4467c92124b3712d05af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3121153a2a914543aaa4b792b1c1f3d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e2954f84e043fe8c68d017fb74e4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1131971e95740e39dc153413fbcf70f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee55f16c06d41d9905d0f935b0ef475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73d1cde0a244fe6aa528793bdc91244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8906f726fb0d4af78b5d7aedf3947750"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5460ef0ab73d4b97a7f41bd893169ce2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e58e61bf065463bbca3ee7ff3ac2aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2a1ecb61b44935bcbe3f7f08885831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6199e1f6a4564f4aa6a1156872cfa895"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# # ==================================================\n# # 2. 영상 읽기 (imageio를 스트리밍 방식으로 사용)\n# # ==================================================\n# video_path = \"/kaggle/input/test-video-njz/FuJ1RiLoq-M.mp4\"  # 영상 경로\n\n# try:\n#     reader = imageio.get_reader(video_path, 'ffmpeg')\n#     meta = reader.get_meta_data()\n#     original_fps = meta['fps']\n#     print(f\"Imageio - Original FPS: {original_fps}\")\n# except Exception as e:\n#     raise Exception(f\"Failed to open video with imageio: {e}\")\n\n# # 원하는 샘플링 설정: 초당 15프레임\n# desired_fps = 15  \n# sampling_interval = int(round(original_fps / desired_fps))\n# print(f\"Sampling interval (in original frames): {sampling_interval}\")\n\n# # ==================================================\n# # 3. 기본 입력 구성 (OPT 기반 EILEV 프롬프트)\n# # ==================================================\n# bos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.pad_token_id\n# pad_token = tokenizer.pad_token_id\n# newline_token = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n# num_query_tokens = 32  # 영상 관련 쿼스트 토큰 개수\n\n# # 기존 프롬프트: 학습 시 사용했던 형식\n# prompt = \"Question: Describe in detail what actions are taking place in the video?\\nAnswer:\"\n# prompt_tokens = tokenizer(prompt, add_special_tokens=False).input_ids\n\n# base_input_ids = [bos_token] + [pad_token] * num_query_tokens + [newline_token] + prompt_tokens\n# base_input_ids = torch.tensor([base_input_ids], dtype=torch.long).to(device)\n# num_tokens = base_input_ids.shape[1]\n\n# base_video_mask = [0] + [1] * num_query_tokens + [0] * (num_tokens - (1 + num_query_tokens))\n# base_video_mask = torch.tensor([base_video_mask], dtype=torch.bool).to(device)\n\n# # ==================================================\n# # 후처리 함수: 불필요한 접두어 제거\n# # ==================================================\n# def clean_caption(text):\n#     # 각 줄마다 \"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\" 등 접두어 제거\n#     lines = text.splitlines()\n#     filtered = [line.strip() for line in lines if not any(line.strip().startswith(prefix) for prefix in \n#                                                            [\"Question:\", \"Answer:\", \"Explanation:\", \"Cameraman:\"])]\n#     return \" \".join(filtered).strip()\n\n# # ==================================================\n# # 후처리 함수: 중복 문장 제거\n# # ==================================================\n# def remove_duplicate_sentences(text):\n#     # 문장을 마침표를 기준으로 분리\n#     sentences = re.split(r'\\.\\s*', text)\n#     seen = set()\n#     unique_sentences = []\n#     for s in sentences:\n#         s = s.strip()\n#         if s and s not in seen:\n#             seen.add(s)\n#             unique_sentences.append(s)\n#     # 중복 제거한 문장을 다시 마침표로 이어 붙임\n#     result = '. '.join(unique_sentences)\n#     if result and not result.endswith('.'):\n#         result += '.'\n#     return result\n\n# # ==================================================\n# # 4. 슬라이딩 윈도우를 이용한 세그먼트 분할 및 캡션 생성 (오버랩 적용)\n# # ==================================================\n# # 세그먼트 설정: 세그먼트 길이를 5초, 오버랩은 1초로 설정\n# segment_duration = 5    # 초 단위\n# overlap_duration = 1    # 초 단위\n\n# frames_per_segment = int(desired_fps * segment_duration)   # 15 fps * 5초 = 75 프레임\n# overlap_frames = int(desired_fps * overlap_duration)         # 15 fps * 1초 = 15 프레임\n# step_frames = frames_per_segment - overlap_frames            # 75 - 15 = 60 프레임\n\n# print(f\"frames_per_segment: {frames_per_segment}, step_frames: {step_frames}\")\n\n# metadata = []\n# current_segment = 0\n# start_time_overall = time.time()\n\n# # deque를 사용하여 슬라이딩 윈도우 구성 (최대 frames_per_segment 개의 프레임 유지)\n# window = deque(maxlen=frames_per_segment)\n# sample_index = 0  # 실제 프레임 번호 = sample_index * sampling_interval\n\n# # 초기 윈도우 채우기 (desired_fps 기반 샘플링)\n# print(\"Preloading initial window...\")\n# for i in range(frames_per_segment):\n#     try:\n#         frame = reader.get_data(i * sampling_interval)\n#         window.append(frame)\n#         sample_index = i + 1\n#     except Exception:\n#         break\n\n# if len(window) < frames_per_segment:\n#     print(\"Not enough frames to form a full segment. Exiting.\")\n# else:\n#     # 첫 세그먼트 처리 (샘플 인덱스 0 ~ frames_per_segment-1)\n#     seg_start_time = 0.0\n#     seg_end_time = frames_per_segment / desired_fps  # 예: 30 / 6 = 5초\n#     segment_frames = list(window)\n    \n#     segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n#     segment_tensor = segment_tensor.unsqueeze(0)  # (1, num_frames, 3, H, W)\n#     segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)  # (1, 3, num_frames, H, W)\n    \n#     inputs_video = process(processor, video=segment_tensor, text=None)\n#     pixel_values = inputs_video[\"pixel_values\"].to(device)\n    \n#     inputs_seg = {\n#         \"input_ids\": base_input_ids,\n#         \"pixel_values\": pixel_values\n#     }\n#     video_input_mask_seg = base_video_mask\n    \n#     print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n#     with torch.no_grad():\n#         output = model.generate(\n#             input_ids=inputs_seg[\"input_ids\"],\n#             pixel_values=inputs_seg[\"pixel_values\"],\n#             video_input_mask=video_input_mask_seg,\n#             max_new_tokens=120,\n#             num_beams=6,\n#             repetition_penalty=1.2,\n#         )\n#     generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n#     # 후처리: 불필요한 접두어 제거 후, 중복 문장 제거\n#     final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n#     print(f\"   Generated caption: {final_caption}\")\n    \n#     metadata.append({\n#         \"segment\": current_segment,\n#         \"start_time\": seg_start_time,\n#         \"end_time\": seg_end_time,\n#         \"caption\": final_caption\n#     })\n#     current_segment += 1\n\n#     # 이후 슬라이딩 윈도우로 세그먼트 처리\n#     while True:\n#         count = 0\n#         while count < step_frames:\n#             try:\n#                 frame = reader.get_data(sample_index * sampling_interval)\n#                 window.append(frame)\n#                 sample_index += 1\n#                 count += 1\n#             except Exception:\n#                 break\n#         if count < step_frames:\n#             print(\"Reached end of video during sliding window update.\")\n#             break\n        \n#         seg_start_time = (sample_index - frames_per_segment) / desired_fps\n#         seg_end_time = sample_index / desired_fps\n#         segment_frames = list(window)\n        \n#         segment_tensor = torch.from_numpy(np.stack(segment_frames)).permute(0, 3, 1, 2)\n#         segment_tensor = segment_tensor.unsqueeze(0)\n#         segment_tensor = segment_tensor.permute(0, 2, 1, 3, 4)\n        \n#         inputs_video = process(processor, video=segment_tensor, text=None)\n#         pixel_values = inputs_video[\"pixel_values\"].to(device)\n        \n#         inputs_seg = {\n#             \"input_ids\": base_input_ids,\n#             \"pixel_values\": pixel_values\n#         }\n#         video_input_mask_seg = base_video_mask\n        \n#         print(f\"Segment {current_segment}: {seg_start_time:.1f}s - {seg_end_time:.1f}s\")\n#         with torch.no_grad():\n#             output = model.generate(\n#                 input_ids=inputs_seg[\"input_ids\"],\n#                 pixel_values=inputs_seg[\"pixel_values\"],\n#                 video_input_mask=video_input_mask_seg,\n#                 max_new_tokens=120,\n#                 num_beams=6,\n#                 repetition_penalty=1.2,\n#             )\n#         generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n#         final_caption = remove_duplicate_sentences(clean_caption(generated_text))\n#         print(f\"   Generated caption: {final_caption}\")\n        \n#         metadata.append({\n#             \"segment\": current_segment,\n#             \"start_time\": seg_start_time,\n#             \"end_time\": seg_end_time,\n#             \"caption\": final_caption\n#         })\n#         current_segment += 1\n\n# total_time = time.time() - start_time_overall\n# print(f\"Total segmentation and caption generation time: {total_time:.1f}s\")\n\n# # ==================================================\n# # 5. 결과 출력 및 파일 저장\n# # ==================================================\n# for seg in metadata:\n#     print(f\"Segment {seg['segment']} ({seg['start_time']:.1f}s - {seg['end_time']:.1f}s): {seg['caption']}\")\n\n# with open(\"/kaggle/working/video_captions_opt.json\", \"w\", encoding=\"utf-8\") as f:\n#     json.dump(metadata, f, indent=4, ensure_ascii=False)\n# print(\"Result saved to '/kaggle/working/video_captions_opt.json'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-18T11:29:19.177Z"}},"outputs":[],"execution_count":null}]}